{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e112c6-7939-4e69-9c08-c8c926e2abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.18\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2389e9-1dcb-478e-bd0d-283e9351ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Librerías para deep learning (simulando con sklearn por compatibilidad)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Deep learning\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "import joblib  # Para guardar modelos como fusion_model o scaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318eb49c-5359-4f94-b1da-baeb86d834f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptedCrimePredictionModel:\n",
    "    \n",
    "    def __init__(self, spatial_clusters=15, temporal_window=30, attention_features=10):\n",
    "        self.spatial_clusters = spatial_clusters\n",
    "        self.temporal_window = temporal_window\n",
    "        self.attention_features = attention_features\n",
    "        \n",
    "        # Componentes del modelo híbrido\n",
    "        self.spatial_clusterer = None\n",
    "        self.temporal_analyzer = None\n",
    "        self.attention_weights = None\n",
    "        self.fusion_model = None\n",
    "        self.rnn_model = None\n",
    "\n",
    "        # Scalers y encoders\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoders = {}\n",
    "\n",
    "        # Métricas de rendimiento\n",
    "        self.performance_metrics = {}\n",
    "        self.num_classes = None\n",
    "        self.is_trained = False\n",
    "\n",
    "    def create_temporal_sequences(self, df, sequence_length=30):\n",
    "        \"\"\"\n",
    "        Crear secuencias temporales de crímenes por cluster espacial\n",
    "        para alimentar una RNN como LSTM o GRU.\n",
    "        \"\"\"\n",
    "        print(\"Creando secuencias temporales para la RNN...\")\n",
    "\n",
    "        df = df.sort_values(['spatial_cluster', 'datetime']).reset_index(drop=True)\n",
    "        sequence_data = []\n",
    "        sequence_labels = []\n",
    "\n",
    "        clusters = df['spatial_cluster'].unique()\n",
    "        for cluster in clusters:\n",
    "            cluster_df = df[df['spatial_cluster'] == cluster]\n",
    "            if len(cluster_df) < sequence_length:\n",
    "                continue\n",
    "\n",
    "            temporal_features = ['crime_count_7d', 'crime_count_30d', 'temporal_weight']\n",
    "\n",
    "            for i in range(len(cluster_df) - sequence_length):\n",
    "                sequence = cluster_df.iloc[i:i+sequence_length][temporal_features].values\n",
    "                label = cluster_df.iloc[i+sequence_length]['NIBRS_Offense_encoded']\n",
    "                sequence_data.append(sequence)\n",
    "                sequence_labels.append(label)\n",
    "\n",
    "        return np.array(sequence_data), np.array(sequence_labels)\n",
    "\n",
    "    def build_rnn_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Construir modelo RNN (LSTM) con API funcional de Keras.\n",
    "        \"\"\"\n",
    "        from tensorflow.keras.models import Model\n",
    "        from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "    \n",
    "        inputs = Input(shape=input_shape, name=\"input_sequence\")\n",
    "        x = LSTM(64, return_sequences=False, name=\"lstm\")(inputs)\n",
    "        x = Dense(64, activation='relu', name=\"dense_embedding\")(x)\n",
    "        outputs = Dense(self.num_classes, activation='softmax', name=\"output\")(x)\n",
    "    \n",
    "        model = Model(inputs=inputs, outputs=outputs, name=\"HybridRNN\")\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def train_rnn_model(self, X_seq, y_seq):\n",
    "        \"\"\"\n",
    "        Entrenar modelo RNN y extraer embeddings.\n",
    "        \"\"\"\n",
    "        from tensorflow.keras.models import Model\n",
    "    \n",
    "        print(\"Entrenando RNN (LSTM)...\")\n",
    "        self.num_classes = len(np.unique(y_seq))\n",
    "        self.rnn_model = self.build_rnn_model(input_shape=(X_seq.shape[1], X_seq.shape[2]))\n",
    "        self.rnn_model.fit(X_seq, y_seq, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "        # Extraer embeddings desde la capa con nombre \"dense_embedding\"\n",
    "        embedding_model = Model(\n",
    "            inputs=self.rnn_model.input,\n",
    "            outputs=self.rnn_model.get_layer(\"dense_embedding\").output\n",
    "        )\n",
    "        embeddings = embedding_model.predict(X_seq)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def train_combined_model(self, X_context, rnn_embeddings, y):\n",
    "        \"\"\"Entrenar modelo combinado (Random Forest + RNN embeddings)\"\"\"\n",
    "        print(\"Entrenando modelo combinado (Random Forest + RNN embeddings)...\")\n",
    "    \n",
    "        # Eliminar columnas datetime\n",
    "        datetime_cols = X_context.select_dtypes(include=['datetime']).columns\n",
    "        if len(datetime_cols) > 0:\n",
    "            print(f\"Eliminando columnas datetime para entrenamiento: {list(datetime_cols)}\")\n",
    "            X_context_clean = X_context.drop(columns=datetime_cols)\n",
    "        else:\n",
    "            X_context_clean = X_context.copy()\n",
    "    \n",
    "        # Escalar características contextuales\n",
    "        X_context_scaled = self.scaler.fit_transform(X_context_clean)\n",
    "        \n",
    "        # Combinar características contextuales escaladas + embeddings RNN\n",
    "        X_combined = np.hstack([X_context_scaled, rnn_embeddings])\n",
    "    \n",
    "        # Configurar y entrenar Random Forest\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100],              # Antes: [100, 200]\n",
    "            'max_depth': [10, None],            # Antes: [10, 20, None]\n",
    "            'min_samples_split': [2],           # Antes: [2, 5]\n",
    "            'min_samples_leaf': [1],            # Antes: [1, 2]\n",
    "            'class_weight': ['balanced']        # Mantenido\n",
    "        }\n",
    "\n",
    "    \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            param_grid=param_grid,\n",
    "            scoring='f1_weighted',\n",
    "            cv=3,\n",
    "            verbose=2,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "        grid_search.fit(X_combined, y)\n",
    "        self.fusion_model = grid_search.best_estimator_\n",
    "        self.is_trained = True\n",
    "    \n",
    "        print(f\"\\n✅ Mejor modelo combinado encontrado: {grid_search.best_params_}\")\n",
    "    \n",
    "    def predict_combined(self, X_context, rnn_embeddings):\n",
    "        \"\"\"Realizar predicciones con el modelo combinado\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"El modelo no ha sido entrenado. Llama a train_combined_model() primero.\")\n",
    "            \n",
    "        # Limpiar columnas datetime\n",
    "        datetime_cols = X_context.select_dtypes(include=['datetime']).columns\n",
    "        if len(datetime_cols) > 0:\n",
    "            X_context_clean = X_context.drop(columns=datetime_cols)\n",
    "        else:\n",
    "            X_context_clean = X_context.copy()\n",
    "            \n",
    "        # Escalar y combinar\n",
    "        X_context_scaled = self.scaler.transform(X_context_clean)\n",
    "        X_combined = np.hstack([X_context_scaled, rnn_embeddings])\n",
    "        \n",
    "        return self.fusion_model.predict(X_combined)\n",
    "    \n",
    "    def evaluate_combined_model(self, X_context, rnn_embeddings, y_true):\n",
    "        \"\"\"Evaluar modelo combinado\"\"\"\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "        \n",
    "        y_pred = self.predict_combined(X_context, rnn_embeddings)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        print(f\"Accuracy combinado: {accuracy:.4f}\")\n",
    "        print(f\"F1-Score combinado: {f1:.4f}\")\n",
    "        \n",
    "        return {\"accuracy\": accuracy, \"f1_score\": f1}\n",
    "    \n",
    "    def full_training_pipeline(self, df_contextual):\n",
    "        \"\"\"Pipeline completo de entrenamiento\"\"\"\n",
    "        print(\"=== Iniciando pipeline de entrenamiento completo ===\")\n",
    "        \n",
    "        # 1. Preparar características contextuales y objetivo\n",
    "        X_context, y_context = self.prepare_features_target(df_contextual)\n",
    "        \n",
    "        # 2. Crear secuencias temporales para RNN\n",
    "        X_seq, y_seq = self.create_temporal_sequences(df_contextual, self.temporal_window)\n",
    "        \n",
    "        if len(X_seq) == 0:\n",
    "            raise ValueError(\"No se pudieron crear secuencias temporales. Verifica los datos.\")\n",
    "        \n",
    "        # 3. Entrenar RNN y extraer embeddings\n",
    "        rnn_embeddings = self.train_rnn_model(X_seq, y_seq)\n",
    "        \n",
    "        # 4. Alinear datos (importante: solo usar los registros que tienen secuencias)\n",
    "        # Necesitamos mapear los embeddings con los datos contextuales correctos\n",
    "        valid_indices = self._get_valid_sequence_indices(df_contextual, self.temporal_window)\n",
    "        X_context_aligned = X_context.iloc[valid_indices].reset_index(drop=True)\n",
    "        y_context_aligned = y_context.iloc[valid_indices].reset_index(drop=True)\n",
    "        \n",
    "        # 5. Aplicar SMOTE para balancear clases\n",
    "        from imblearn.over_sampling import BorderlineSMOTE\n",
    "        \n",
    "        # Eliminar columnas datetime antes de SMOTE\n",
    "        datetime_cols = X_context_aligned.select_dtypes(include=['datetime']).columns\n",
    "        X_context_clean = X_context_aligned.drop(columns=datetime_cols) if len(datetime_cols) > 0 else X_context_aligned\n",
    "        \n",
    "        # Escalar antes de SMOTE\n",
    "        X_context_scaled = self.scaler.fit_transform(X_context_clean)\n",
    "        X_combined = np.hstack([X_context_scaled, rnn_embeddings])\n",
    "        \n",
    "        smote = BorderlineSMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_combined, y_context_aligned)\n",
    "        \n",
    "        # 6. Entrenar modelo final con datos balanceados\n",
    "        # Separar de nuevo las características contextuales y embeddings\n",
    "        n_context_features = X_context_scaled.shape[1]\n",
    "        X_context_resampled = X_resampled[:, :n_context_features]\n",
    "        rnn_embeddings_resampled = X_resampled[:, n_context_features:]\n",
    "        \n",
    "        # Crear DataFrame para compatibilidad\n",
    "        X_context_resampled_df = pd.DataFrame(X_context_resampled, columns=X_context_clean.columns)\n",
    "        \n",
    "        self.train_combined_model(X_context_resampled_df, rnn_embeddings_resampled, y_resampled)\n",
    "        \n",
    "        print(\"✅ Pipeline de entrenamiento completado exitosamente\")\n",
    "        return X_context_aligned, rnn_embeddings, y_context_aligned\n",
    "\n",
    "    def _get_valid_sequence_indices(self, df, sequence_length):\n",
    "        \"\"\"Obtener índices válidos que tienen secuencias temporales correspondientes\"\"\"\n",
    "        df_sorted = df.sort_values(['spatial_cluster', 'datetime']).reset_index(drop=True)\n",
    "        valid_indices = []\n",
    "        \n",
    "        clusters = df_sorted['spatial_cluster'].unique()\n",
    "        for cluster in clusters:\n",
    "            cluster_df = df_sorted[df_sorted['spatial_cluster'] == cluster]\n",
    "            if len(cluster_df) < sequence_length:\n",
    "                continue\n",
    "                \n",
    "            # Los índices válidos son desde sequence_length hasta el final\n",
    "            cluster_indices = cluster_df.index[sequence_length:].tolist()\n",
    "            valid_indices.extend(cluster_indices)\n",
    "            \n",
    "        return valid_indices\n",
    "\n",
    "    def predict_future_crimes(self, df_contextual, days_ahead=7):\n",
    "        \"\"\"Predecir crímenes futuros usando el modelo entrenado\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"El modelo no ha sido entrenado.\")\n",
    "            \n",
    "        print(f\"\\n🔮 Prediciendo crímenes para los próximos {days_ahead} días...\")\n",
    "        future_data = []\n",
    "    \n",
    "        for cluster_id in df_contextual['spatial_cluster'].unique():\n",
    "            cluster_df = df_contextual[df_contextual['spatial_cluster'] == cluster_id]\n",
    "            cluster_df = cluster_df.sort_values('datetime')\n",
    "    \n",
    "            # Verificar que hay suficientes datos\n",
    "            if len(cluster_df) < self.temporal_window:\n",
    "                continue\n",
    "    \n",
    "            # Usar los datos más recientes para crear secuencia\n",
    "            recent_seq = cluster_df.tail(self.temporal_window)\n",
    "            temporal_features = ['crime_count_7d', 'crime_count_30d', 'temporal_weight']\n",
    "            sequence = recent_seq[temporal_features].values[np.newaxis, :, :]\n",
    "    \n",
    "            # Extraer embedding RNN\n",
    "            from tensorflow.keras.models import Model\n",
    "            intermediate_layer = Model(inputs=self.rnn_model.input, outputs=self.rnn_model.layers[-2].output)\n",
    "            embedding = intermediate_layer.predict(sequence, verbose=0)\n",
    "    \n",
    "            # Preparar características contextuales\n",
    "            context_features = ['crime_severity_score', 'temporal_weight', 'lat', 'lon'] \n",
    "            # Usar las características disponibles\n",
    "            available_context = [col for col in context_features if col in recent_seq.columns]\n",
    "            if not available_context:\n",
    "                # Usar características por defecto si no están disponibles\n",
    "                available_context = [col for col in recent_seq.columns if col in self.scaler.feature_names_in_][:4]\n",
    "            \n",
    "            if available_context:\n",
    "                context_vector_df = pd.DataFrame([recent_seq[available_context].mean()])\n",
    "            else:\n",
    "                # Crear contexto mínimo si no hay características disponibles\n",
    "                context_vector_df = pd.DataFrame([[0, 0, 0, 0]], columns=['feat1', 'feat2', 'feat3', 'feat4'])\n",
    "    \n",
    "            # Realizar predicción\n",
    "            try:\n",
    "                prediction = self.predict_combined(context_vector_df, embedding)\n",
    "                future_data.append({\n",
    "                    'spatial_cluster': cluster_id,\n",
    "                    'predicted_crime': prediction[0]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error prediciendo para cluster {cluster_id}: {e}\")\n",
    "                continue\n",
    "    \n",
    "        return pd.DataFrame(future_data)\n",
    "\n",
    "\n",
    "    def preprocess_real_data(self, df):\n",
    "        \"\"\"\n",
    "        Preprocesamiento específico para el dataset real\n",
    "        \"\"\"\n",
    "        print(\"Iniciando preprocesamiento de datos reales...\")\n",
    "        \n",
    "        # Crear copia para evitar modificar el DataFrame original\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Limpiar y convertir fechas\n",
    "        df['ReportDate'] = pd.to_datetime(df['ReportDate'], errors='coerce')\n",
    "        df['OccurredFromDate'] = pd.to_datetime(df['OccurredFromDate'], errors='coerce')\n",
    "        df['OccurredToDate'] = pd.to_datetime(df['OccurredToDate'], errors='coerce')\n",
    "        \n",
    "        # Usar OccurredFromDate como fecha principal\n",
    "        df['datetime'] = df['OccurredFromDate'].fillna(df['ReportDate'])\n",
    "        \n",
    "        # Eliminar registros sin fecha válida\n",
    "        df = df.dropna(subset=['datetime']).reset_index(drop=True)\n",
    "        \n",
    "        # Crear características temporales\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['quarter'] = df['datetime'].dt.quarter\n",
    "        \n",
    "        # Características cíclicas (importante para patrones temporales)\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        \n",
    "        # Codificar variables categóricas importantes\n",
    "        categorical_columns = ['NIBRS_Offense', 'Part', 'Crime_Against', 'LocationType', \n",
    "                             'Zone', 'DISTRICT', 'NPU', 'NhoodName', 'Beat']\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            if col in df.columns:\n",
    "                # Limpiar datos categóricos\n",
    "                df[col] = df[col].astype(str).fillna('Unknown')\n",
    "                le = LabelEncoder()\n",
    "                df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "                self.label_encoders[col] = le\n",
    "        \n",
    "        # Convertir variables booleanas de manera más robusta\n",
    "        if 'FireArmInvolved' in df.columns:\n",
    "            df['FireArmInvolved_binary'] = df['FireArmInvolved'].astype(str).str.lower().map({\n",
    "                'yes': 1, 'true': 1, '1': 1, 'y': 1\n",
    "            }).fillna(0).astype(int)\n",
    "        else:\n",
    "            df['FireArmInvolved_binary'] = 0\n",
    "            \n",
    "        if 'IsBiasMotivationInvolved' in df.columns:\n",
    "            df['IsBiasMotivationInvolved_binary'] = pd.to_numeric(\n",
    "                df['IsBiasMotivationInvolved'], errors='coerce'\n",
    "            ).fillna(0).astype(int)\n",
    "        else:\n",
    "            df['IsBiasMotivationInvolved_binary'] = 0\n",
    "        \n",
    "        # Limpiar coordenadas (eliminar valores extremos)\n",
    "        if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "            df = df[(df['Latitude'] > 30) & (df['Latitude'] < 40)]  # Rango válido para Atlanta\n",
    "            df = df[(df['Longitude'] > -90) & (df['Longitude'] < -80)]\n",
    "            df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Asegurar que Vic_Count sea numérico\n",
    "        if 'Vic_Count' in df.columns:\n",
    "            df['Vic_Count'] = pd.to_numeric(df['Vic_Count'], errors='coerce').fillna(1)\n",
    "        else:\n",
    "            df['Vic_Count'] = 1\n",
    "        \n",
    "        # Crear Zone_int si no existe\n",
    "        if 'Zone' in df.columns and 'Zone_int' not in df.columns:\n",
    "            df['Zone_int'] = df['Zone_encoded']\n",
    "        elif 'Zone_int' not in df.columns:\n",
    "            df['Zone_int'] = 0\n",
    "        \n",
    "        print(f\"Datos preprocesados: {df.shape[0]} registros, {df.shape[1]} características\")\n",
    "        return df\n",
    "    \n",
    "    def create_crime_severity_score(self, df):\n",
    "        \"\"\"\n",
    "        Crear un score de severidad del crimen basado en múltiples factores\n",
    "        \"\"\"\n",
    "        print(\"Creando score de severidad del crimen...\")\n",
    "        \n",
    "        # Score base por tipo de crimen (Part I crimes son más severos)\n",
    "        if 'Part' in df.columns:\n",
    "            df['part_severity'] = df['Part'].map({'Part I': 2, 'Part II': 1}).fillna(1)\n",
    "        else:\n",
    "            df['part_severity'] = 1\n",
    "        \n",
    "        # Score por arma de fuego involucrada\n",
    "        df['firearm_severity'] = df['FireArmInvolved_binary'] * 2\n",
    "        \n",
    "        # Score por víctimas\n",
    "        df['victim_severity'] = np.log1p(df['Vic_Count'])\n",
    "        \n",
    "        # Score por motivación de odio\n",
    "        df['bias_severity'] = df['IsBiasMotivationInvolved_binary'] * 1.5\n",
    "        \n",
    "        # Score combinado\n",
    "        df['crime_severity_score'] = (df['part_severity'] + \n",
    "                                    df['firearm_severity'] + \n",
    "                                    df['victim_severity'] + \n",
    "                                    df['bias_severity'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def enhanced_spatial_clustering(self, df):\n",
    "        \"\"\"\n",
    "        Clustering espacial mejorado usando información geográfica real\n",
    "        \"\"\"\n",
    "        print(\"Ejecutando clustering espacial mejorado...\")\n",
    "        \n",
    "        if 'Latitude' not in df.columns or 'Longitude' not in df.columns:\n",
    "            print(\"Coordenadas no disponibles, usando clustering simple\")\n",
    "            df['spatial_cluster'] = 0\n",
    "            return df\n",
    "        \n",
    "        # Usar coordenadas reales\n",
    "        spatial_features = df[['Latitude', 'Longitude']].values\n",
    "        \n",
    "        # DBSCAN con parámetros ajustados para datos reales de ciudad\n",
    "        dbscan = DBSCAN(eps=0.005, min_samples=10)\n",
    "        spatial_clusters = dbscan.fit_predict(spatial_features)\n",
    "        \n",
    "        # K-means como respaldo\n",
    "        kmeans = KMeans(n_clusters=self.spatial_clusters, random_state=42, n_init=10)\n",
    "        kmeans_clusters = kmeans.fit_predict(spatial_features)\n",
    "        \n",
    "        # Combinar resultados\n",
    "        final_clusters = spatial_clusters.copy()\n",
    "        outlier_mask = (spatial_clusters == -1)\n",
    "        if np.any(outlier_mask):\n",
    "            max_cluster = spatial_clusters.max() if spatial_clusters.max() >= 0 else 0\n",
    "            final_clusters[outlier_mask] = kmeans_clusters[outlier_mask] + max_cluster + 1\n",
    "        \n",
    "        df['spatial_cluster'] = final_clusters\n",
    "        self.spatial_clusterer = {'dbscan': dbscan, 'kmeans': kmeans}\n",
    "        \n",
    "        # Agregar información de zona administrativa como característica\n",
    "        df['zone_cluster_consistency'] = (df['Zone_int'] == df['spatial_cluster']).astype(int)\n",
    "        \n",
    "        print(f\"Identificados {len(np.unique(final_clusters))} clusters espaciales\")\n",
    "        return df\n",
    "\n",
    "    def advanced_temporal_analysis(self, df):\n",
    "        \"\"\"\n",
    "        Análisis temporal avanzado usando los campos de fecha reales\n",
    "        \"\"\"\n",
    "        print(\"Ejecutando análisis temporal avanzado...\")\n",
    "        \n",
    "        # Crear copia y asegurar orden temporal\n",
    "        df_sorted = df.copy().sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        # Calcular conteos temporales de manera más robusta\n",
    "        try:\n",
    "            # Convertir a DataFrame con índice temporal para rolling\n",
    "            df_temp = df_sorted.set_index('datetime')\n",
    "            \n",
    "            # Calcular rolling counts por cluster espacial\n",
    "            rolling_counts = []\n",
    "            for cluster in df_sorted['spatial_cluster'].unique():\n",
    "                cluster_data = df_temp[df_temp['spatial_cluster'] == cluster]\n",
    "                \n",
    "                if len(cluster_data) > 0:\n",
    "                    # Calcular conteos rolling\n",
    "                    rolling_7d = cluster_data['OBJECTID'].rolling('7D').count()\n",
    "                    rolling_30d = cluster_data['OBJECTID'].rolling('30D').count()\n",
    "                    \n",
    "                    # Crear DataFrame temporal\n",
    "                    temp_df = pd.DataFrame({\n",
    "                        'datetime': rolling_7d.index,\n",
    "                        'spatial_cluster': cluster,\n",
    "                        'crime_count_7d': rolling_7d.values,\n",
    "                        'crime_count_30d': rolling_30d.values\n",
    "                    })\n",
    "                    rolling_counts.append(temp_df)\n",
    "            \n",
    "            # Combinar todos los conteos\n",
    "            if rolling_counts:\n",
    "                all_rolling = pd.concat(rolling_counts, ignore_index=True)\n",
    "                \n",
    "                # Hacer merge con el DataFrame original\n",
    "                df_sorted = df_sorted.merge(\n",
    "                    all_rolling, \n",
    "                    on=['datetime', 'spatial_cluster'], \n",
    "                    how='left'\n",
    "                )\n",
    "            else:\n",
    "                df_sorted['crime_count_7d'] = 1\n",
    "                df_sorted['crime_count_30d'] = 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error en análisis temporal: {e}\")\n",
    "            df_sorted['crime_count_7d'] = 1\n",
    "            df_sorted['crime_count_30d'] = 1\n",
    "        \n",
    "        # Rellenar valores faltantes\n",
    "        df_sorted['crime_count_7d'] = df_sorted['crime_count_7d'].fillna(1)\n",
    "        df_sorted['crime_count_30d'] = df_sorted['crime_count_30d'].fillna(1)\n",
    "        \n",
    "        # Tiempo entre reporte y ocurrencia\n",
    "        if 'ReportDate' in df_sorted.columns:\n",
    "            df_sorted['report_delay_hours'] = (\n",
    "                df_sorted['ReportDate'] - df_sorted['OccurredFromDate']\n",
    "            ).dt.total_seconds() / 3600\n",
    "            df_sorted['report_delay_hours'] = df_sorted['report_delay_hours'].fillna(0).clip(0, 24 * 30)\n",
    "        else:\n",
    "            df_sorted['report_delay_hours'] = 0\n",
    "        \n",
    "        # Duración del incidente\n",
    "        if 'OccurredToDate' in df_sorted.columns:\n",
    "            df_sorted['incident_duration_hours'] = (\n",
    "                df_sorted['OccurredToDate'] - df_sorted['OccurredFromDate']\n",
    "            ).dt.total_seconds() / 3600\n",
    "            df_sorted['incident_duration_hours'] = df_sorted['incident_duration_hours'].fillna(0).clip(0, 24)\n",
    "        else:\n",
    "            df_sorted['incident_duration_hours'] = 0\n",
    "        \n",
    "        # Pesos de atención temporal\n",
    "        df_sorted['temporal_weight'] = self._calculate_attention_weights(df_sorted)\n",
    "\n",
    "        df_sorted['temporal_series'] = df_sorted.apply(lambda row: [\n",
    "            row['crime_count_7d'],\n",
    "            row['crime_count_30d'],\n",
    "            row['temporal_weight']\n",
    "        ], axis=1)\n",
    "        \n",
    "        return df_sorted\n",
    "    \n",
    "    def _calculate_attention_weights(self, df):\n",
    "        \"\"\"\n",
    "        Calcular pesos de atención basados en recencia y patrones\n",
    "        \"\"\"\n",
    "        current_time = df['datetime'].max()\n",
    "        time_diff = (current_time - df['datetime']).dt.days\n",
    "        \n",
    "        # Peso exponencial decreciente con factor ajustado\n",
    "        attention_weights = np.exp(-time_diff / 60)  # Decaimiento de 60 días\n",
    "        \n",
    "        # Ajustar por severidad del crimen\n",
    "        if 'crime_severity_score' in df.columns:\n",
    "            severity_factor = df['crime_severity_score'] / df['crime_severity_score'].max()\n",
    "            attention_weights = attention_weights * (1 + severity_factor)\n",
    "        \n",
    "        # Normalizar por cluster espacial\n",
    "        df_temp = df.copy()\n",
    "        df_temp['temp_weights'] = attention_weights\n",
    "        \n",
    "        try:\n",
    "            normalized_weights = df_temp.groupby('spatial_cluster')['temp_weights'].transform(\n",
    "                lambda x: x / x.sum() if x.sum() > 0 else x\n",
    "            )\n",
    "        except:\n",
    "            normalized_weights = attention_weights\n",
    "        \n",
    "        return normalized_weights\n",
    "    \n",
    "    def create_contextual_features(self, df):\n",
    "        \"\"\"\n",
    "        Crear características contextuales usando información del dataset\n",
    "        \"\"\"\n",
    "        print(\"Creando características contextuales avanzadas...\")\n",
    "        \n",
    "        # Crear copia para evitar modificaciones indeseadas\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Densidad de crímenes por zona/beat\n",
    "        if 'Zone' in df.columns:\n",
    "            crime_density_zone = df.groupby('Zone')['OBJECTID'].count()\n",
    "            df['zone_crime_density'] = df['Zone'].map(crime_density_zone).fillna(0)\n",
    "        else:\n",
    "            df['zone_crime_density'] = 0\n",
    "        \n",
    "        if 'Beat' in df.columns:\n",
    "            crime_density_beat = df.groupby('Beat')['OBJECTID'].count()\n",
    "            df['beat_crime_density'] = df['Beat'].map(crime_density_beat).fillna(0)\n",
    "        else:\n",
    "            df['beat_crime_density'] = 0\n",
    "        \n",
    "        # Características de ubicación específica\n",
    "        if 'LocationType' in df.columns:\n",
    "            location_crime_count = df.groupby('LocationType')['OBJECTID'].count()\n",
    "            df['location_type_frequency'] = df['LocationType'].map(location_crime_count).fillna(0)\n",
    "        else:\n",
    "            df['location_type_frequency'] = 0\n",
    "        \n",
    "        # Distancia al centro de la ciudad\n",
    "        if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "            center_lat = df['Latitude'].median()\n",
    "            center_lon = df['Longitude'].median()\n",
    "            df['distance_to_center'] = np.sqrt(\n",
    "                (df['Latitude'] - center_lat)**2 + (df['Longitude'] - center_lon)**2\n",
    "            )\n",
    "        else:\n",
    "            df['distance_to_center'] = 0\n",
    "        \n",
    "        # Características de actividad mediática (si existen)\n",
    "        df['has_press_release'] = 0\n",
    "        df['has_social_media'] = 0\n",
    "        df['media_attention'] = 0\n",
    "        \n",
    "        if 'press_release' in df.columns:\n",
    "            df['has_press_release'] = df['press_release'].notna().astype(int)\n",
    "        if 'social_media' in df.columns:\n",
    "            df['has_social_media'] = df['social_media'].notna().astype(int)\n",
    "        \n",
    "        df['media_attention'] = df['has_press_release'] + df['has_social_media']\n",
    "        \n",
    "        # Índice de actividad nocturna\n",
    "        df['nighttime_activity'] = np.where((df['hour'] >= 22) | (df['hour'] <= 6), 1, 0)\n",
    "        \n",
    "        # Fin de semana\n",
    "        df['is_weekend'] = np.where(df['day_of_week'].isin([5, 6]), 1, 0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_features_target(self, df):\n",
    "        \"\"\"\n",
    "        Preparar características y variable objetivo para el modelo\n",
    "        \"\"\"\n",
    "        print(\"Preparando características y variable objetivo...\")\n",
    "        \n",
    "        # Características numéricas base\n",
    "        base_numeric_features = [\n",
    "            'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "            'month_sin', 'month_cos', 'spatial_cluster', 'temporal_weight',\n",
    "            'crime_count_7d', 'crime_count_30d', 'crime_severity_score',\n",
    "            'zone_crime_density', 'beat_crime_density', 'location_type_frequency',\n",
    "            'distance_to_center', 'nighttime_activity', 'is_weekend',\n",
    "            'report_delay_hours', 'incident_duration_hours', 'media_attention',\n",
    "            'FireArmInvolved_binary', 'IsBiasMotivationInvolved_binary',\n",
    "            'Vic_Count', 'Zone_int'\n",
    "        ]\n",
    "        \n",
    "        # Agregar coordenadas si están disponibles\n",
    "        if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "            base_numeric_features.extend(['Latitude', 'Longitude'])\n",
    "        \n",
    "        # Características categóricas codificadas\n",
    "        categorical_features = [col for col in df.columns if col.endswith('_encoded')]\n",
    "        \n",
    "        # Combinar todas las características\n",
    "        all_features = base_numeric_features + categorical_features\n",
    "        \n",
    "        # Filtrar características que realmente existen en el dataset\n",
    "        available_features = [col for col in all_features if col in df.columns]\n",
    "        \n",
    "        # Asegurar que tenemos la variable objetivo\n",
    "        if 'NIBRS_Offense_encoded' not in df.columns:\n",
    "            raise ValueError(\"Variable objetivo 'NIBRS_Offense_encoded' no encontrada\")\n",
    "        \n",
    "        # Preparar datos\n",
    "        feature_cols = available_features\n",
    "\n",
    "        # Evitar que la variable objetivo esté duplicada en las características\n",
    "        if 'NIBRS_Offense_encoded' in feature_cols:\n",
    "            feature_cols.remove('NIBRS_Offense_encoded')\n",
    "        \n",
    "        df_clean = df[feature_cols + ['NIBRS_Offense_encoded']].copy()\n",
    "\n",
    "        \n",
    "        # Rellenar valores faltantes\n",
    "        for col in df_clean.columns:\n",
    "            if col != 'NIBRS_Offense_encoded':\n",
    "                if df_clean[col].dtype in ['float64', 'int64']:\n",
    "                    df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "                else:\n",
    "                    df_clean[col] = df_clean[col].fillna(0)\n",
    "        \n",
    "        # Preparar X e y\n",
    "        X = df_clean[feature_cols]\n",
    "        y = df_clean['NIBRS_Offense_encoded'].astype(int)\n",
    "        \n",
    "        print(f\"Características preparadas: {X.shape[1]} variables\")\n",
    "        print(f\"Clases objetivo: {len(np.unique(y))} tipos de crimen diferentes\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_fusion_model(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Entrenar modelo de fusión optimizado\n",
    "        \"\"\"\n",
    "        print(\"Entrenando modelo de fusión híbrido optimizado...\")\n",
    "        \n",
    "        # Normalizar características\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Modelo Random Forest optimizado para clasificación multiclase\n",
    "        self.fusion_model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=3,\n",
    "            max_features='sqrt',\n",
    "            bootstrap=True,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.fusion_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Calcular importancia de características\n",
    "        feature_importance = self.fusion_model.feature_importances_\n",
    "        self.attention_weights = dict(zip(X_train.columns, feature_importance))\n",
    "        \n",
    "        print(\"Modelo híbrido entrenado exitosamente\")\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Realizar predicciones con intervalos de confianza\n",
    "        \"\"\"\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        predictions = self.fusion_model.predict(X_test_scaled)\n",
    "        probabilities = self.fusion_model.predict_proba(X_test_scaled)\n",
    "        \n",
    "        # Calcular confianza de predicción\n",
    "        confidence_scores = np.max(probabilities, axis=1)\n",
    "        \n",
    "        return predictions, probabilities, confidence_scores\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluación completa del modelo\n",
    "        \"\"\"\n",
    "        print(\"Evaluando rendimiento del modelo...\")\n",
    "        \n",
    "        predictions, probabilities, confidence_scores = self.predict(X_test)\n",
    "        \n",
    "        # Métricas de clasificación\n",
    "        precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "        accuracy = self.fusion_model.score(self.scaler.transform(X_test), y_test)\n",
    "        \n",
    "        self.performance_metrics = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'mean_confidence': np.mean(confidence_scores)\n",
    "        }\n",
    "        \n",
    "        print(f\"Precisión: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        print(f\"Exactitud: {accuracy:.4f}\")\n",
    "        print(f\"Confianza promedio: {np.mean(confidence_scores):.4f}\")\n",
    "\n",
    "        print(\"\\n=== Classification Report ===\")\n",
    "        print(classification_report(y_test, predictions, zero_division=0))\n",
    "        \n",
    "        # Matriz de confusión normalizada\n",
    "        cm = confusion_matrix(y_test, predictions, normalize='true')\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, cmap='Blues')\n",
    "        plt.title(\"Matriz de Confusión Normalizada\")\n",
    "        plt.xlabel(\"Predicho\")\n",
    "        plt.ylabel(\"Real\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self.performance_metrics\n",
    "    \n",
    "    def visualize_results(self, df):\n",
    "        \"\"\"\n",
    "        Visualizaciones comprehensivas de los resultados\n",
    "        \"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(20, 15))\n",
    "            \n",
    "            # Subplot 1: Mapa de calor de crímenes\n",
    "            plt.subplot(3, 4, 1)\n",
    "            if 'Longitude' in df.columns and 'Latitude' in df.columns:\n",
    "                plt.scatter(df['Longitude'], df['Latitude'], \n",
    "                           c=df['spatial_cluster'], cmap='viridis', alpha=0.6, s=0.5)\n",
    "                plt.colorbar()\n",
    "                plt.title('Clusters Espaciales de Crímenes')\n",
    "                plt.xlabel('Longitud')\n",
    "                plt.ylabel('Latitud')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Coordenadas no disponibles', ha='center', va='center')\n",
    "                plt.title('Clusters Espaciales')\n",
    "            \n",
    "            # Subplot 2: Distribución temporal por hora\n",
    "            plt.subplot(3, 4, 2)\n",
    "            hourly_crimes = df.groupby('hour').size()\n",
    "            plt.bar(hourly_crimes.index, hourly_crimes.values, alpha=0.7)\n",
    "            plt.title('Distribución de Crímenes por Hora')\n",
    "            plt.xlabel('Hora del Día')\n",
    "            plt.ylabel('Número de Crímenes')\n",
    "            \n",
    "            # Subplot 3: Crímenes por día de la semana\n",
    "            plt.subplot(3, 4, 3)\n",
    "            daily_crimes = df.groupby('day_of_week').size()\n",
    "            days = ['Lun', 'Mar', 'Mié', 'Jue', 'Vie', 'Sáb', 'Dom']\n",
    "            plt.bar(range(len(daily_crimes)), daily_crimes.values, alpha=0.7)\n",
    "            plt.xticks(range(len(daily_crimes)), [days[i] for i in daily_crimes.index])\n",
    "            plt.title('Crímenes por Día de la Semana')\n",
    "            plt.xlabel('Día de la Semana')\n",
    "            plt.ylabel('Número de Crímenes')\n",
    "            \n",
    "            # Subplot 4: Top tipos de crimen\n",
    "            plt.subplot(3, 4, 4)\n",
    "            if 'NIBRS_Offense' in df.columns:\n",
    "                top_crimes = df['NIBRS_Offense'].value_counts().head(8)\n",
    "                plt.barh(range(len(top_crimes)), top_crimes.values)\n",
    "                plt.yticks(range(len(top_crimes)), [str(x)[:20] for x in top_crimes.index], fontsize=8)\n",
    "                plt.title('Top 8 Tipos de Crimen')\n",
    "                plt.xlabel('Frecuencia')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Tipos de crimen no disponibles', ha='center', va='center')\n",
    "                plt.title('Tipos de Crimen')\n",
    "            \n",
    "            # Subplot 5: Distribución de severidad\n",
    "            plt.subplot(3, 4, 5)\n",
    "            plt.hist(df['crime_severity_score'], bins=30, alpha=0.7)\n",
    "            plt.title('Distribución de Severidad del Crimen')\n",
    "            plt.xlabel('Score de Severidad')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            \n",
    "            # Subplot 6: Pesos de atención temporal\n",
    "            plt.subplot(3, 4, 6)\n",
    "            plt.hist(df['temporal_weight'], bins=50, alpha=0.7)\n",
    "            plt.title('Pesos de Atención Temporal')\n",
    "            plt.xlabel('Peso de Atención')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            \n",
    "            # Subplot 7: Crímenes por zona\n",
    "            plt.subplot(3, 4, 7)\n",
    "            if 'Zone' in df.columns:\n",
    "                zone_crimes = df.groupby('Zone').size().sort_values(ascending=False).head(10)\n",
    "                plt.bar(range(len(zone_crimes)), zone_crimes.values, alpha=0.7)\n",
    "                plt.xticks(range(len(zone_crimes)), zone_crimes.index, rotation=45, fontsize=8)\n",
    "                plt.title('Crímenes por Zona (Top 10)')\n",
    "                plt.ylabel('Número de Crímenes')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Zonas no disponibles', ha='center', va='center')\n",
    "                plt.title('Crímenes por Zona')\n",
    "            \n",
    "            # Subplot 8: Armas de fuego involucradas\n",
    "            plt.subplot(3, 4, 8)\n",
    "            firearm_dist = df['FireArmInvolved_binary'].value_counts()\n",
    "            labels = ['No', 'Sí']\n",
    "            plt.pie(firearm_dist.values, labels=labels, autopct='%1.1f%%')\n",
    "            plt.title('Armas de Fuego Involucradas')\n",
    "            \n",
    "            # Subplot 9: Tendencia temporal mensual\n",
    "            plt.subplot(3, 4, 9)\n",
    "            monthly_trend = df.groupby(df['datetime'].dt.to_period('M')).size()\n",
    "            if len(monthly_trend) > 1:\n",
    "                plt.plot(range(len(monthly_trend)), monthly_trend.values)\n",
    "                plt.title('Tendencia Mensual de Crímenes')\n",
    "                plt.xlabel('Período')\n",
    "                plt.ylabel('Número de Crímenes')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Datos insuficientes para tendencia', ha='center', va='center')\n",
    "                plt.title('Tendencia Mensual')\n",
    "            \n",
    "            # Subplot 10: Actividad por clusters\n",
    "            plt.subplot(3, 4, 10)\n",
    "            cluster_activity = df.groupby('spatial_cluster').size()\n",
    "            plt.bar(cluster_activity.index, cluster_activity.values, alpha=0.7)\n",
    "            plt.title('Actividad por Cluster Espacial')\n",
    "            plt.xlabel('Cluster')\n",
    "            plt.ylabel('Número de Crímenes')\n",
    "            \n",
    "            # Subplot 11: Actividad nocturna vs diurna\n",
    "            plt.subplot(3, 4, 11)\n",
    "            night_activity = df['nighttime_activity'].value_counts()\n",
    "            labels = ['Diurno', 'Nocturno']\n",
    "            plt.pie(night_activity.values, labels=labels, autopct='%1.1f%%')\n",
    "            plt.title('Actividad Nocturna vs Diurna')\n",
    "            \n",
    "            # Subplot 12: Importancia de características\n",
    "            plt.subplot(3, 4, 12)\n",
    "            if self.attention_weights:\n",
    "                importance_df = pd.DataFrame(\n",
    "                    list(self.attention_weights.items()),\n",
    "                    columns=['Feature', 'Importance']\n",
    "                ).sort_values('Importance', ascending=False).head(10)\n",
    "                \n",
    "                plt.barh(range(len(importance_df)), importance_df['Importance'].values)\n",
    "                plt.yticks(range(len(importance_df)), \n",
    "                          [str(x)[:15] for x in importance_df['Feature'].values], fontsize=8)\n",
    "                plt.title('Top 10 Características Importantes')\n",
    "                plt.xlabel('Importancia')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Modelo no entrenado', ha='center', va='center')\n",
    "                plt.title('Importancia de Características')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en visualización: {e}\")\n",
    "            print(\"Continuando sin visualizaciones...\")\n",
    "    def prepare_hybrid_training_data(self, df):\n",
    "        \"\"\"\n",
    "        Prepara los datos para el entrenamiento híbrido:\n",
    "        - X_context: características contextuales (entrenadas con RandomForest)\n",
    "        - embeddings: representaciones embebidas de series temporales (para RNN)\n",
    "        - y_context: etiquetas a predecir\n",
    "    \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame procesado con características contextuales y temporales\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, pd.Series]: X_context, embeddings, y_context\n",
    "        \"\"\"\n",
    "        # Verificar que temporal_series exista\n",
    "        if 'temporal_series' not in df.columns:\n",
    "            raise ValueError(\"Falta la columna 'temporal_series'. Asegúrate de que 'advanced_temporal_analysis' la haya generado.\")\n",
    "    \n",
    "        # === Construir embeddings desde temporal_series ===\n",
    "        embeddings = df['temporal_series'].apply(lambda ts: np.array(ts)).tolist()\n",
    "        embeddings = pd.DataFrame(embeddings)\n",
    "    \n",
    "        # === Etiqueta objetivo ===\n",
    "        y_context = df['NIBRS_Offense_encoded']\n",
    "    \n",
    "        # === Seleccionar y limpiar características contextuales ===\n",
    "        exclude_cols = ['NIBRS_Offense_encoded', 'temporal_series', 'ReportDate', 'OccurredFromDate', 'OccurredToDate', 'datetime',\n",
    "                        'IncidentNumber', 'ReportNumber', 'ChargeId']\n",
    "        \n",
    "        X_context = df.drop(columns=[col for col in exclude_cols if col in df.columns], errors='ignore')\n",
    "    \n",
    "        # Codificar columnas categóricas no numéricas (por ejemplo, 'Day_of_the_week' si aún está como string)\n",
    "        for col in X_context.select_dtypes(include=['object', 'string']).columns:\n",
    "            if X_context[col].nunique() < 100:\n",
    "                X_context[col] = LabelEncoder().fit_transform(X_context[col].astype(str))\n",
    "            else:\n",
    "                print(f\"⚠️ Columna '{col}' tiene demasiados valores únicos. Considera eliminarla.\")\n",
    "                X_context = X_context.drop(columns=[col])  # Se descarta por seguridad\n",
    "    \n",
    "        return X_context, embeddings, y_context\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Obtener ranking de importancia de características\n",
    "        \"\"\"\n",
    "        if self.attention_weights:\n",
    "            importance_df = pd.DataFrame(\n",
    "                list(self.attention_weights.items()),\n",
    "                columns=['Feature', 'Importance']\n",
    "            ).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            return importance_df\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac15f4df-285e-4f2c-a72f-f9a103258604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_with_real_data(csv_file_path):\n",
    "    print(\"=== Sistema de Predicción Criminal con Datos Reales ===\\n\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Cargando datos desde: {csv_file_path}\")\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        print(f\"Dataset cargado: {df.shape[0]} registros, {df.shape[1]} campos\\n\")\n",
    "        df['OccurredFromDate'] = pd.to_datetime(df['OccurredFromDate'], errors='coerce')\n",
    "        df['OccurredToDate'] = pd.to_datetime(df['OccurredToDate'], errors='coerce')\n",
    "\n",
    "        print(\"Primeras columnas del dataset:\")\n",
    "        print(df.columns.tolist()[:10])\n",
    "        print(f\"\\nTamaño del dataset: {df.shape}\")\n",
    "        print(f\"Rango de fechas: {df['OccurredFromDate'].min()} a {df['OccurredFromDate'].max()}\")\n",
    "\n",
    "        model = AdaptedCrimePredictionModel(\n",
    "            spatial_clusters=20,\n",
    "            temporal_window=30,\n",
    "            attention_features=15\n",
    "        )\n",
    "\n",
    "        print(\"\\nIniciando pipeline de procesamiento...\\n\")\n",
    "\n",
    "        df_processed = model.preprocess_real_data(df)\n",
    "        df_severity = model.create_crime_severity_score(df_processed)\n",
    "        df_clustered = model.enhanced_spatial_clustering(df_severity)\n",
    "        df_temporal = model.advanced_temporal_analysis(df_clustered)\n",
    "        df_contextual = model.create_contextual_features(df_temporal)\n",
    "\n",
    "        # === Crear secuencias para RNN ===\n",
    "        X_seq, y_seq = model.create_temporal_sequences(df_contextual, model.temporal_window)\n",
    "        if len(X_seq) == 0:\n",
    "            raise ValueError(\"No se pudieron crear secuencias temporales. Verifica tus datos.\")\n",
    "        \n",
    "        # === Entrenar RNN y extraer embeddings ===\n",
    "        rnn_embeddings = model.train_rnn_model(X_seq, y_seq)\n",
    "\n",
    "        # === Preparar características contextuales ===\n",
    "        X_context, _, y_context = model.prepare_hybrid_training_data(df_contextual)\n",
    "        valid_indices = model._get_valid_sequence_indices(df_contextual, model.temporal_window)\n",
    "        X_context = X_context.iloc[valid_indices].reset_index(drop=True)\n",
    "        y_context = pd.Series(y_context).iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "        # Alinear embeddings\n",
    "        embeddings = pd.DataFrame(rnn_embeddings)\n",
    "\n",
    "        # Diagnóstico de clases\n",
    "        class_counts = y_context.value_counts().sort_index()\n",
    "        print(\"Distribución de clases:\")\n",
    "        print(class_counts)\n",
    "\n",
    "        imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "        print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\n",
    "        plt.title(\"Distribución de clases en 'NIBRS_Offense_encoded'\")\n",
    "        plt.xlabel(\"Clase (tipo de crimen codificado)\")\n",
    "        plt.ylabel(\"Número de registros\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Filtrar clases con al menos 2 muestras\n",
    "        valid_classes = class_counts[class_counts >= 2].index\n",
    "        mask = y_context.isin(valid_classes)\n",
    "        X_context = X_context[mask].reset_index(drop=True)\n",
    "        embeddings = embeddings[mask].reset_index(drop=True)\n",
    "        y_context = y_context[mask].reset_index(drop=True)\n",
    "\n",
    "        # Separar en conjunto de entrenamiento y prueba\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        X_train, X_test, emb_train, emb_test, y_train, y_test = train_test_split(\n",
    "            X_context, embeddings, y_context,\n",
    "            test_size=0.2, stratify=y_context, random_state=42\n",
    "        )\n",
    "\n",
    "        print(\"\\nEntrenando modelo combinado...\")\n",
    "        model.train_combined_model(X_train, emb_train, y_train)\n",
    "\n",
    "        print(\"\\nEvaluando modelo combinado...\")\n",
    "        metrics = model.evaluate_combined_model(X_test, emb_test, y_test)\n",
    "\n",
    "        print(\"\\nTop 15 Características más Importantes:\")\n",
    "        feature_importance = model.get_feature_importance() if hasattr(model, 'get_feature_importance') else None\n",
    "        if feature_importance is not None:\n",
    "            print(feature_importance.head(15))\n",
    "\n",
    "        print(\"\\n=== Resumen del Modelo ===\")\n",
    "        print(f\"Registros procesados: {len(df_contextual):,}\")\n",
    "        print(f\"Clusters espaciales: {len(np.unique(df_contextual['spatial_cluster']))}\")\n",
    "        print(f\"Precisión del modelo: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "        print(\"\\n=== Entrenamiento Completado Exitosamente ===\")\n",
    "\n",
    "        # Exportar modelos\n",
    "        export_dir = \"exported_models\"\n",
    "        os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "        joblib.dump(model.fusion_model, os.path.join(export_dir, \"fusion_model.pkl\"))\n",
    "        print(\"✅ Modelo fusion ok.\")\n",
    "        joblib.dump((scaler, X_train.columns.tolist()), \"exported_models/scaler.pkl\")\n",
    "        print(\"✅ Escaler ok.\")\n",
    "        model.rnn_model.save(os.path.join(export_dir, \"rnn_model.h5\"))\n",
    "        model.rnn_model.save(os.path.join(export_dir, \"rnn_model.keras\"))\n",
    "        print(\"✅ rnn ok.\")\n",
    "\n",
    "        print(\"✅ Modelos exportados correctamente.\")\n",
    "\n",
    "        return model, df_contextual, metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el procesamiento: {str(e)}\")\n",
    "        print(\"Verificar que el archivo CSV existe y tiene el formato correcto.\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4098deb-922d-4feb-8993-b0ab48f6e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sistema de Predicción Criminal con Datos Reales ===\n",
      "\n",
      "Cargando datos desde: AxonCrimeData_Export_WA_1686331975127960619 (1).csv\n",
      "Dataset cargado: 261177 registros, 32 campos\n",
      "\n",
      "Primeras columnas del dataset:\n",
      "['OBJECTID', 'ChargeId', 'IncidentNumber', 'ReportNumber', 'FireArmInvolved', 'ReportDate', 'OccurredFromDate', 'OccurredToDate', 'Day_of_the_week', 'Day_Number']\n",
      "\n",
      "Tamaño del dataset: (261177, 32)\n",
      "Rango de fechas: 1924-08-21 20:39:00 a 2025-06-18 12:01:00\n",
      "\n",
      "Iniciando pipeline de procesamiento...\n",
      "\n",
      "Iniciando preprocesamiento de datos reales...\n",
      "Datos preprocesados: 261175 registros, 56 características\n",
      "Creando score de severidad del crimen...\n",
      "Ejecutando clustering espacial mejorado...\n",
      "Identificados 21 clusters espaciales\n",
      "Ejecutando análisis temporal avanzado...\n",
      "Creando características contextuales avanzadas...\n",
      "Creando secuencias temporales para la RNN...\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Reemplazar con la ruta real de tu archivo CSV\n",
    "    csv_path = \"AxonCrimeData_Export_WA_1686331975127960619 (1).csv\"\n",
    "    \n",
    "    # Para usar el código, descomenta la siguiente línea y ajusta la ruta:\n",
    "    trained_model, processed_data, performance_metrics = main_with_real_data(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e9780-1d7c-43cb-8ef8-dd0e53dd9fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd92759-bcd4-46fa-99d5-f4cc53f7b37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499bb88f-f92f-45eb-89da-da2e57c2ba2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7006f-2d37-4739-befd-255b8af18204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
