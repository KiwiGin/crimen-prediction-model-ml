{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e112c6-7939-4e69-9c08-c8c926e2abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.18\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2389e9-1dcb-478e-bd0d-283e9351ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Librer√≠as para deep learning (simulando con sklearn por compatibilidad)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Deep learning\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "import joblib  # Para guardar modelos como fusion_model o scaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318eb49c-5359-4f94-b1da-baeb86d834f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptedCrimePredictionModel:\n",
    "    \n",
    "    def __init__(self, spatial_clusters=15, temporal_window=30, attention_features=10):\n",
    "        self.spatial_clusters = spatial_clusters\n",
    "        self.temporal_window = temporal_window\n",
    "        self.attention_features = attention_features\n",
    "        \n",
    "        # Componentes del modelo h√≠brido\n",
    "        self.spatial_clusterer = None\n",
    "        self.temporal_analyzer = None\n",
    "        self.attention_weights = None\n",
    "        self.fusion_model = None\n",
    "        self.rnn_model = None\n",
    "\n",
    "        # Scalers y encoders\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoders = {}\n",
    "\n",
    "        # M√©tricas de rendimiento\n",
    "        self.performance_metrics = {}\n",
    "        self.num_classes = None\n",
    "        self.is_trained = False\n",
    "\n",
    "    def create_temporal_sequences(self, df, sequence_length=30):\n",
    "        \"\"\"\n",
    "        Crear secuencias temporales de cr√≠menes por cluster espacial\n",
    "        para alimentar una RNN como LSTM o GRU.\n",
    "        \"\"\"\n",
    "        print(\"Creando secuencias temporales para la RNN...\")\n",
    "\n",
    "        df = df.sort_values(['spatial_cluster', 'datetime']).reset_index(drop=True)\n",
    "        sequence_data = []\n",
    "        sequence_labels = []\n",
    "\n",
    "        clusters = df['spatial_cluster'].unique()\n",
    "        for cluster in clusters:\n",
    "            cluster_df = df[df['spatial_cluster'] == cluster]\n",
    "            if len(cluster_df) < sequence_length:\n",
    "                continue\n",
    "\n",
    "            temporal_features = ['crime_count_7d', 'crime_count_30d', 'temporal_weight']\n",
    "\n",
    "            for i in range(len(cluster_df) - sequence_length):\n",
    "                sequence = cluster_df.iloc[i:i+sequence_length][temporal_features].values\n",
    "                label = cluster_df.iloc[i+sequence_length]['NIBRS_Offense_encoded']\n",
    "                sequence_data.append(sequence)\n",
    "                sequence_labels.append(label)\n",
    "\n",
    "        return np.array(sequence_data), np.array(sequence_labels)\n",
    "\n",
    "    def build_rnn_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Construir modelo RNN (LSTM) con API funcional de Keras.\n",
    "        \"\"\"\n",
    "        from tensorflow.keras.models import Model\n",
    "        from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "    \n",
    "        inputs = Input(shape=input_shape, name=\"input_sequence\")\n",
    "        x = LSTM(64, return_sequences=False, name=\"lstm\")(inputs)\n",
    "        x = Dense(64, activation='relu', name=\"dense_embedding\")(x)\n",
    "        outputs = Dense(self.num_classes, activation='softmax', name=\"output\")(x)\n",
    "    \n",
    "        model = Model(inputs=inputs, outputs=outputs, name=\"HybridRNN\")\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def train_rnn_model(self, X_seq, y_seq):\n",
    "        \"\"\"\n",
    "        Entrenar modelo RNN y extraer embeddings.\n",
    "        \"\"\"\n",
    "        from tensorflow.keras.models import Model\n",
    "    \n",
    "        print(\"Entrenando RNN (LSTM)...\")\n",
    "        self.num_classes = len(np.unique(y_seq))\n",
    "        self.rnn_model = self.build_rnn_model(input_shape=(X_seq.shape[1], X_seq.shape[2]))\n",
    "        self.rnn_model.fit(X_seq, y_seq, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "        # Extraer embeddings desde la capa con nombre \"dense_embedding\"\n",
    "        embedding_model = Model(\n",
    "            inputs=self.rnn_model.input,\n",
    "            outputs=self.rnn_model.get_layer(\"dense_embedding\").output\n",
    "        )\n",
    "        embeddings = embedding_model.predict(X_seq)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def train_combined_model(self, X_context, rnn_embeddings, y):\n",
    "        \"\"\"Entrenar modelo combinado (Random Forest + RNN embeddings)\"\"\"\n",
    "        print(\"Entrenando modelo combinado (Random Forest + RNN embeddings)...\")\n",
    "    \n",
    "        # Eliminar columnas datetime\n",
    "        datetime_cols = X_context.select_dtypes(include=['datetime']).columns\n",
    "        if len(datetime_cols) > 0:\n",
    "            print(f\"Eliminando columnas datetime para entrenamiento: {list(datetime_cols)}\")\n",
    "            X_context_clean = X_context.drop(columns=datetime_cols)\n",
    "        else:\n",
    "            X_context_clean = X_context.copy()\n",
    "    \n",
    "        # Escalar caracter√≠sticas contextuales\n",
    "        X_context_scaled = self.scaler.fit_transform(X_context_clean)\n",
    "        \n",
    "        # Combinar caracter√≠sticas contextuales escaladas + embeddings RNN\n",
    "        X_combined = np.hstack([X_context_scaled, rnn_embeddings])\n",
    "    \n",
    "        # Configurar y entrenar Random Forest\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100],              # Antes: [100, 200]\n",
    "            'max_depth': [10, None],            # Antes: [10, 20, None]\n",
    "            'min_samples_split': [2],           # Antes: [2, 5]\n",
    "            'min_samples_leaf': [1],            # Antes: [1, 2]\n",
    "            'class_weight': ['balanced']        # Mantenido\n",
    "        }\n",
    "\n",
    "    \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            param_grid=param_grid,\n",
    "            scoring='f1_weighted',\n",
    "            cv=3,\n",
    "            verbose=2,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "        grid_search.fit(X_combined, y)\n",
    "        self.fusion_model = grid_search.best_estimator_\n",
    "        self.is_trained = True\n",
    "    \n",
    "        print(f\"\\n‚úÖ Mejor modelo combinado encontrado: {grid_search.best_params_}\")\n",
    "    \n",
    "    def predict_combined(self, X_context, rnn_embeddings):\n",
    "        \"\"\"Realizar predicciones con el modelo combinado\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"El modelo no ha sido entrenado. Llama a train_combined_model() primero.\")\n",
    "            \n",
    "        # Limpiar columnas datetime\n",
    "        datetime_cols = X_context.select_dtypes(include=['datetime']).columns\n",
    "        if len(datetime_cols) > 0:\n",
    "            X_context_clean = X_context.drop(columns=datetime_cols)\n",
    "        else:\n",
    "            X_context_clean = X_context.copy()\n",
    "            \n",
    "        # Escalar y combinar\n",
    "        X_context_scaled = self.scaler.transform(X_context_clean)\n",
    "        X_combined = np.hstack([X_context_scaled, rnn_embeddings])\n",
    "        \n",
    "        return self.fusion_model.predict(X_combined)\n",
    "    \n",
    "    def evaluate_combined_model(self, X_context, rnn_embeddings, y_true):\n",
    "        \"\"\"Evaluar modelo combinado\"\"\"\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "        \n",
    "        y_pred = self.predict_combined(X_context, rnn_embeddings)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        print(f\"Accuracy combinado: {accuracy:.4f}\")\n",
    "        print(f\"F1-Score combinado: {f1:.4f}\")\n",
    "        \n",
    "        return {\"accuracy\": accuracy, \"f1_score\": f1}\n",
    "    \n",
    "    def full_training_pipeline(self, df_contextual):\n",
    "        \"\"\"Pipeline completo de entrenamiento\"\"\"\n",
    "        print(\"=== Iniciando pipeline de entrenamiento completo ===\")\n",
    "        \n",
    "        # 1. Preparar caracter√≠sticas contextuales y objetivo\n",
    "        X_context, y_context = self.prepare_features_target(df_contextual)\n",
    "        \n",
    "        # 2. Crear secuencias temporales para RNN\n",
    "        X_seq, y_seq = self.create_temporal_sequences(df_contextual, self.temporal_window)\n",
    "        \n",
    "        if len(X_seq) == 0:\n",
    "            raise ValueError(\"No se pudieron crear secuencias temporales. Verifica los datos.\")\n",
    "        \n",
    "        # 3. Entrenar RNN y extraer embeddings\n",
    "        rnn_embeddings = self.train_rnn_model(X_seq, y_seq)\n",
    "        \n",
    "        # 4. Alinear datos (importante: solo usar los registros que tienen secuencias)\n",
    "        # Necesitamos mapear los embeddings con los datos contextuales correctos\n",
    "        valid_indices = self._get_valid_sequence_indices(df_contextual, self.temporal_window)\n",
    "        X_context_aligned = X_context.iloc[valid_indices].reset_index(drop=True)\n",
    "        y_context_aligned = y_context.iloc[valid_indices].reset_index(drop=True)\n",
    "        \n",
    "        # 5. Aplicar SMOTE para balancear clases\n",
    "        from imblearn.over_sampling import BorderlineSMOTE\n",
    "        \n",
    "        # Eliminar columnas datetime antes de SMOTE\n",
    "        datetime_cols = X_context_aligned.select_dtypes(include=['datetime']).columns\n",
    "        X_context_clean = X_context_aligned.drop(columns=datetime_cols) if len(datetime_cols) > 0 else X_context_aligned\n",
    "        \n",
    "        # Escalar antes de SMOTE\n",
    "        X_context_scaled = self.scaler.fit_transform(X_context_clean)\n",
    "        X_combined = np.hstack([X_context_scaled, rnn_embeddings])\n",
    "        \n",
    "        smote = BorderlineSMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_combined, y_context_aligned)\n",
    "        \n",
    "        # 6. Entrenar modelo final con datos balanceados\n",
    "        # Separar de nuevo las caracter√≠sticas contextuales y embeddings\n",
    "        n_context_features = X_context_scaled.shape[1]\n",
    "        X_context_resampled = X_resampled[:, :n_context_features]\n",
    "        rnn_embeddings_resampled = X_resampled[:, n_context_features:]\n",
    "        \n",
    "        # Crear DataFrame para compatibilidad\n",
    "        X_context_resampled_df = pd.DataFrame(X_context_resampled, columns=X_context_clean.columns)\n",
    "        \n",
    "        self.train_combined_model(X_context_resampled_df, rnn_embeddings_resampled, y_resampled)\n",
    "        \n",
    "        print(\"‚úÖ Pipeline de entrenamiento completado exitosamente\")\n",
    "        return X_context_aligned, rnn_embeddings, y_context_aligned\n",
    "\n",
    "    def _get_valid_sequence_indices(self, df, sequence_length):\n",
    "        \"\"\"Obtener √≠ndices v√°lidos que tienen secuencias temporales correspondientes\"\"\"\n",
    "        df_sorted = df.sort_values(['spatial_cluster', 'datetime']).reset_index(drop=True)\n",
    "        valid_indices = []\n",
    "        \n",
    "        clusters = df_sorted['spatial_cluster'].unique()\n",
    "        for cluster in clusters:\n",
    "            cluster_df = df_sorted[df_sorted['spatial_cluster'] == cluster]\n",
    "            if len(cluster_df) < sequence_length:\n",
    "                continue\n",
    "                \n",
    "            # Los √≠ndices v√°lidos son desde sequence_length hasta el final\n",
    "            cluster_indices = cluster_df.index[sequence_length:].tolist()\n",
    "            valid_indices.extend(cluster_indices)\n",
    "            \n",
    "        return valid_indices\n",
    "\n",
    "    def predict_future_crimes(self, df_contextual, days_ahead=7):\n",
    "        \"\"\"Predecir cr√≠menes futuros usando el modelo entrenado\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"El modelo no ha sido entrenado.\")\n",
    "            \n",
    "        print(f\"\\nüîÆ Prediciendo cr√≠menes para los pr√≥ximos {days_ahead} d√≠as...\")\n",
    "        future_data = []\n",
    "    \n",
    "        for cluster_id in df_contextual['spatial_cluster'].unique():\n",
    "            cluster_df = df_contextual[df_contextual['spatial_cluster'] == cluster_id]\n",
    "            cluster_df = cluster_df.sort_values('datetime')\n",
    "    \n",
    "            # Verificar que hay suficientes datos\n",
    "            if len(cluster_df) < self.temporal_window:\n",
    "                continue\n",
    "    \n",
    "            # Usar los datos m√°s recientes para crear secuencia\n",
    "            recent_seq = cluster_df.tail(self.temporal_window)\n",
    "            temporal_features = ['crime_count_7d', 'crime_count_30d', 'temporal_weight']\n",
    "            sequence = recent_seq[temporal_features].values[np.newaxis, :, :]\n",
    "    \n",
    "            # Extraer embedding RNN\n",
    "            from tensorflow.keras.models import Model\n",
    "            intermediate_layer = Model(inputs=self.rnn_model.input, outputs=self.rnn_model.layers[-2].output)\n",
    "            embedding = intermediate_layer.predict(sequence, verbose=0)\n",
    "    \n",
    "            # Preparar caracter√≠sticas contextuales\n",
    "            context_features = ['crime_severity_score', 'temporal_weight', 'lat', 'lon'] \n",
    "            # Usar las caracter√≠sticas disponibles\n",
    "            available_context = [col for col in context_features if col in recent_seq.columns]\n",
    "            if not available_context:\n",
    "                # Usar caracter√≠sticas por defecto si no est√°n disponibles\n",
    "                available_context = [col for col in recent_seq.columns if col in self.scaler.feature_names_in_][:4]\n",
    "            \n",
    "            if available_context:\n",
    "                context_vector_df = pd.DataFrame([recent_seq[available_context].mean()])\n",
    "            else:\n",
    "                # Crear contexto m√≠nimo si no hay caracter√≠sticas disponibles\n",
    "                context_vector_df = pd.DataFrame([[0, 0, 0, 0]], columns=['feat1', 'feat2', 'feat3', 'feat4'])\n",
    "    \n",
    "            # Realizar predicci√≥n\n",
    "            try:\n",
    "                prediction = self.predict_combined(context_vector_df, embedding)\n",
    "                future_data.append({\n",
    "                    'spatial_cluster': cluster_id,\n",
    "                    'predicted_crime': prediction[0]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error prediciendo para cluster {cluster_id}: {e}\")\n",
    "                continue\n",
    "    \n",
    "        return pd.DataFrame(future_data)\n",
    "\n",
    "\n",
    "    def preprocess_real_data(self, df):\n",
    "        \"\"\"\n",
    "        Preprocesamiento espec√≠fico para el dataset real\n",
    "        \"\"\"\n",
    "        print(\"Iniciando preprocesamiento de datos reales...\")\n",
    "        \n",
    "        # Crear copia para evitar modificar el DataFrame original\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Limpiar y convertir fechas\n",
    "        df['ReportDate'] = pd.to_datetime(df['ReportDate'], errors='coerce')\n",
    "        df['OccurredFromDate'] = pd.to_datetime(df['OccurredFromDate'], errors='coerce')\n",
    "        df['OccurredToDate'] = pd.to_datetime(df['OccurredToDate'], errors='coerce')\n",
    "        \n",
    "        # Usar OccurredFromDate como fecha principal\n",
    "        df['datetime'] = df['OccurredFromDate'].fillna(df['ReportDate'])\n",
    "        \n",
    "        # Eliminar registros sin fecha v√°lida\n",
    "        df = df.dropna(subset=['datetime']).reset_index(drop=True)\n",
    "        \n",
    "        # Crear caracter√≠sticas temporales\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['quarter'] = df['datetime'].dt.quarter\n",
    "        \n",
    "        # Caracter√≠sticas c√≠clicas (importante para patrones temporales)\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        \n",
    "        # Codificar variables categ√≥ricas importantes\n",
    "        categorical_columns = ['NIBRS_Offense', 'Part', 'Crime_Against', 'LocationType', \n",
    "                             'Zone', 'DISTRICT', 'NPU', 'NhoodName', 'Beat']\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            if col in df.columns:\n",
    "                # Limpiar datos categ√≥ricos\n",
    "                df[col] = df[col].astype(str).fillna('Unknown')\n",
    "                le = LabelEncoder()\n",
    "                df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "                self.label_encoders[col] = le\n",
    "        \n",
    "        # Convertir variables booleanas de manera m√°s robusta\n",
    "        if 'FireArmInvolved' in df.columns:\n",
    "            df['FireArmInvolved_binary'] = df['FireArmInvolved'].astype(str).str.lower().map({\n",
    "                'yes': 1, 'true': 1, '1': 1, 'y': 1\n",
    "            }).fillna(0).astype(int)\n",
    "        else:\n",
    "            df['FireArmInvolved_binary'] = 0\n",
    "            \n",
    "        if 'IsBiasMotivationInvolved' in df.columns:\n",
    "            df['IsBiasMotivationInvolved_binary'] = pd.to_numeric(\n",
    "                df['IsBiasMotivationInvolved'], errors='coerce'\n",
    "            ).fillna(0).astype(int)\n",
    "        else:\n",
    "            df['IsBiasMotivationInvolved_binary'] = 0\n",
    "        \n",
    "        # Limpiar coordenadas (eliminar valores extremos)\n",
    "        if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "            df = df[(df['Latitude'] > 30) & (df['Latitude'] < 40)]  # Rango v√°lido para Atlanta\n",
    "            df = df[(df['Longitude'] > -90) & (df['Longitude'] < -80)]\n",
    "            df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Asegurar que Vic_Count sea num√©rico\n",
    "        if 'Vic_Count' in df.columns:\n",
    "            df['Vic_Count'] = pd.to_numeric(df['Vic_Count'], errors='coerce').fillna(1)\n",
    "        else:\n",
    "            df['Vic_Count'] = 1\n",
    "        \n",
    "        # Crear Zone_int si no existe\n",
    "        if 'Zone' in df.columns and 'Zone_int' not in df.columns:\n",
    "            df['Zone_int'] = df['Zone_encoded']\n",
    "        elif 'Zone_int' not in df.columns:\n",
    "            df['Zone_int'] = 0\n",
    "        \n",
    "        print(f\"Datos preprocesados: {df.shape[0]} registros, {df.shape[1]} caracter√≠sticas\")\n",
    "        return df\n",
    "    \n",
    "    def create_crime_severity_score(self, df):\n",
    "        \"\"\"\n",
    "        Crear un score de severidad del crimen basado en m√∫ltiples factores\n",
    "        \"\"\"\n",
    "        print(\"Creando score de severidad del crimen...\")\n",
    "        \n",
    "        # Score base por tipo de crimen (Part I crimes son m√°s severos)\n",
    "        if 'Part' in df.columns:\n",
    "            df['part_severity'] = df['Part'].map({'Part I': 2, 'Part II': 1}).fillna(1)\n",
    "        else:\n",
    "            df['part_severity'] = 1\n",
    "        \n",
    "        # Score por arma de fuego involucrada\n",
    "        df['firearm_severity'] = df['FireArmInvolved_binary'] * 2\n",
    "        \n",
    "        # Score por v√≠ctimas\n",
    "        df['victim_severity'] = np.log1p(df['Vic_Count'])\n",
    "        \n",
    "        # Score por motivaci√≥n de odio\n",
    "        df['bias_severity'] = df['IsBiasMotivationInvolved_binary'] * 1.5\n",
    "        \n",
    "        # Score combinado\n",
    "        df['crime_severity_score'] = (df['part_severity'] + \n",
    "                                    df['firearm_severity'] + \n",
    "                                    df['victim_severity'] + \n",
    "                                    df['bias_severity'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def enhanced_spatial_clustering(self, df):\n",
    "        \"\"\"\n",
    "        Clustering espacial mejorado usando informaci√≥n geogr√°fica real\n",
    "        \"\"\"\n",
    "        print(\"Ejecutando clustering espacial mejorado...\")\n",
    "        \n",
    "        if 'Latitude' not in df.columns or 'Longitude' not in df.columns:\n",
    "            print(\"Coordenadas no disponibles, usando clustering simple\")\n",
    "            df['spatial_cluster'] = 0\n",
    "            return df\n",
    "        \n",
    "        # Usar coordenadas reales\n",
    "        spatial_features = df[['Latitude', 'Longitude']].values\n",
    "        \n",
    "        # DBSCAN con par√°metros ajustados para datos reales de ciudad\n",
    "        dbscan = DBSCAN(eps=0.005, min_samples=10)\n",
    "        spatial_clusters = dbscan.fit_predict(spatial_features)\n",
    "        \n",
    "        # K-means como respaldo\n",
    "        kmeans = KMeans(n_clusters=self.spatial_clusters, random_state=42, n_init=10)\n",
    "        kmeans_clusters = kmeans.fit_predict(spatial_features)\n",
    "        \n",
    "        # Combinar resultados\n",
    "        final_clusters = spatial_clusters.copy()\n",
    "        outlier_mask = (spatial_clusters == -1)\n",
    "        if np.any(outlier_mask):\n",
    "            max_cluster = spatial_clusters.max() if spatial_clusters.max() >= 0 else 0\n",
    "            final_clusters[outlier_mask] = kmeans_clusters[outlier_mask] + max_cluster + 1\n",
    "        \n",
    "        df['spatial_cluster'] = final_clusters\n",
    "        self.spatial_clusterer = {'dbscan': dbscan, 'kmeans': kmeans}\n",
    "        \n",
    "        # Agregar informaci√≥n de zona administrativa como caracter√≠stica\n",
    "        df['zone_cluster_consistency'] = (df['Zone_int'] == df['spatial_cluster']).astype(int)\n",
    "        \n",
    "        print(f\"Identificados {len(np.unique(final_clusters))} clusters espaciales\")\n",
    "        return df\n",
    "\n",
    "    def advanced_temporal_analysis(self, df):\n",
    "        \"\"\"\n",
    "        An√°lisis temporal avanzado usando los campos de fecha reales\n",
    "        \"\"\"\n",
    "        print(\"Ejecutando an√°lisis temporal avanzado...\")\n",
    "        \n",
    "        # Crear copia y asegurar orden temporal\n",
    "        df_sorted = df.copy().sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        # Calcular conteos temporales de manera m√°s robusta\n",
    "        try:\n",
    "            # Convertir a DataFrame con √≠ndice temporal para rolling\n",
    "            df_temp = df_sorted.set_index('datetime')\n",
    "            \n",
    "            # Calcular rolling counts por cluster espacial\n",
    "            rolling_counts = []\n",
    "            for cluster in df_sorted['spatial_cluster'].unique():\n",
    "                cluster_data = df_temp[df_temp['spatial_cluster'] == cluster]\n",
    "                \n",
    "                if len(cluster_data) > 0:\n",
    "                    # Calcular conteos rolling\n",
    "                    rolling_7d = cluster_data['OBJECTID'].rolling('7D').count()\n",
    "                    rolling_30d = cluster_data['OBJECTID'].rolling('30D').count()\n",
    "                    \n",
    "                    # Crear DataFrame temporal\n",
    "                    temp_df = pd.DataFrame({\n",
    "                        'datetime': rolling_7d.index,\n",
    "                        'spatial_cluster': cluster,\n",
    "                        'crime_count_7d': rolling_7d.values,\n",
    "                        'crime_count_30d': rolling_30d.values\n",
    "                    })\n",
    "                    rolling_counts.append(temp_df)\n",
    "            \n",
    "            # Combinar todos los conteos\n",
    "            if rolling_counts:\n",
    "                all_rolling = pd.concat(rolling_counts, ignore_index=True)\n",
    "                \n",
    "                # Hacer merge con el DataFrame original\n",
    "                df_sorted = df_sorted.merge(\n",
    "                    all_rolling, \n",
    "                    on=['datetime', 'spatial_cluster'], \n",
    "                    how='left'\n",
    "                )\n",
    "            else:\n",
    "                df_sorted['crime_count_7d'] = 1\n",
    "                df_sorted['crime_count_30d'] = 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error en an√°lisis temporal: {e}\")\n",
    "            df_sorted['crime_count_7d'] = 1\n",
    "            df_sorted['crime_count_30d'] = 1\n",
    "        \n",
    "        # Rellenar valores faltantes\n",
    "        df_sorted['crime_count_7d'] = df_sorted['crime_count_7d'].fillna(1)\n",
    "        df_sorted['crime_count_30d'] = df_sorted['crime_count_30d'].fillna(1)\n",
    "        \n",
    "        # Tiempo entre reporte y ocurrencia\n",
    "        if 'ReportDate' in df_sorted.columns:\n",
    "            df_sorted['report_delay_hours'] = (\n",
    "                df_sorted['ReportDate'] - df_sorted['OccurredFromDate']\n",
    "            ).dt.total_seconds() / 3600\n",
    "            df_sorted['report_delay_hours'] = df_sorted['report_delay_hours'].fillna(0).clip(0, 24 * 30)\n",
    "        else:\n",
    "            df_sorted['report_delay_hours'] = 0\n",
    "        \n",
    "        # Duraci√≥n del incidente\n",
    "        if 'OccurredToDate' in df_sorted.columns:\n",
    "            df_sorted['incident_duration_hours'] = (\n",
    "                df_sorted['OccurredToDate'] - df_sorted['OccurredFromDate']\n",
    "            ).dt.total_seconds() / 3600\n",
    "            df_sorted['incident_duration_hours'] = df_sorted['incident_duration_hours'].fillna(0).clip(0, 24)\n",
    "        else:\n",
    "            df_sorted['incident_duration_hours'] = 0\n",
    "        \n",
    "        # Pesos de atenci√≥n temporal\n",
    "        df_sorted['temporal_weight'] = self._calculate_attention_weights(df_sorted)\n",
    "\n",
    "        df_sorted['temporal_series'] = df_sorted.apply(lambda row: [\n",
    "            row['crime_count_7d'],\n",
    "            row['crime_count_30d'],\n",
    "            row['temporal_weight']\n",
    "        ], axis=1)\n",
    "        \n",
    "        return df_sorted\n",
    "    \n",
    "    def _calculate_attention_weights(self, df):\n",
    "        \"\"\"\n",
    "        Calcular pesos de atenci√≥n basados en recencia y patrones\n",
    "        \"\"\"\n",
    "        current_time = df['datetime'].max()\n",
    "        time_diff = (current_time - df['datetime']).dt.days\n",
    "        \n",
    "        # Peso exponencial decreciente con factor ajustado\n",
    "        attention_weights = np.exp(-time_diff / 60)  # Decaimiento de 60 d√≠as\n",
    "        \n",
    "        # Ajustar por severidad del crimen\n",
    "        if 'crime_severity_score' in df.columns:\n",
    "            severity_factor = df['crime_severity_score'] / df['crime_severity_score'].max()\n",
    "            attention_weights = attention_weights * (1 + severity_factor)\n",
    "        \n",
    "        # Normalizar por cluster espacial\n",
    "        df_temp = df.copy()\n",
    "        df_temp['temp_weights'] = attention_weights\n",
    "        \n",
    "        try:\n",
    "            normalized_weights = df_temp.groupby('spatial_cluster')['temp_weights'].transform(\n",
    "                lambda x: x / x.sum() if x.sum() > 0 else x\n",
    "            )\n",
    "        except:\n",
    "            normalized_weights = attention_weights\n",
    "        \n",
    "        return normalized_weights\n",
    "    \n",
    "    def create_contextual_features(self, df):\n",
    "        \"\"\"\n",
    "        Crear caracter√≠sticas contextuales usando informaci√≥n del dataset\n",
    "        \"\"\"\n",
    "        print(\"Creando caracter√≠sticas contextuales avanzadas...\")\n",
    "        \n",
    "        # Crear copia para evitar modificaciones indeseadas\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Densidad de cr√≠menes por zona/beat\n",
    "        if 'Zone' in df.columns:\n",
    "            crime_density_zone = df.groupby('Zone')['OBJECTID'].count()\n",
    "            df['zone_crime_density'] = df['Zone'].map(crime_density_zone).fillna(0)\n",
    "        else:\n",
    "            df['zone_crime_density'] = 0\n",
    "        \n",
    "        if 'Beat' in df.columns:\n",
    "            crime_density_beat = df.groupby('Beat')['OBJECTID'].count()\n",
    "            df['beat_crime_density'] = df['Beat'].map(crime_density_beat).fillna(0)\n",
    "        else:\n",
    "            df['beat_crime_density'] = 0\n",
    "        \n",
    "        # Caracter√≠sticas de ubicaci√≥n espec√≠fica\n",
    "        if 'LocationType' in df.columns:\n",
    "            location_crime_count = df.groupby('LocationType')['OBJECTID'].count()\n",
    "            df['location_type_frequency'] = df['LocationType'].map(location_crime_count).fillna(0)\n",
    "        else:\n",
    "            df['location_type_frequency'] = 0\n",
    "        \n",
    "        # Distancia al centro de la ciudad\n",
    "        if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "            center_lat = df['Latitude'].median()\n",
    "            center_lon = df['Longitude'].median()\n",
    "            df['distance_to_center'] = np.sqrt(\n",
    "                (df['Latitude'] - center_lat)**2 + (df['Longitude'] - center_lon)**2\n",
    "            )\n",
    "        else:\n",
    "            df['distance_to_center'] = 0\n",
    "        \n",
    "        # Caracter√≠sticas de actividad medi√°tica (si existen)\n",
    "        df['has_press_release'] = 0\n",
    "        df['has_social_media'] = 0\n",
    "        df['media_attention'] = 0\n",
    "        \n",
    "        if 'press_release' in df.columns:\n",
    "            df['has_press_release'] = df['press_release'].notna().astype(int)\n",
    "        if 'social_media' in df.columns:\n",
    "            df['has_social_media'] = df['social_media'].notna().astype(int)\n",
    "        \n",
    "        df['media_attention'] = df['has_press_release'] + df['has_social_media']\n",
    "        \n",
    "        # √çndice de actividad nocturna\n",
    "        df['nighttime_activity'] = np.where((df['hour'] >= 22) | (df['hour'] <= 6), 1, 0)\n",
    "        \n",
    "        # Fin de semana\n",
    "        df['is_weekend'] = np.where(df['day_of_week'].isin([5, 6]), 1, 0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_features_target(self, df):\n",
    "        \"\"\"\n",
    "        Preparar caracter√≠sticas y variable objetivo para el modelo\n",
    "        \"\"\"\n",
    "        print(\"Preparando caracter√≠sticas y variable objetivo...\")\n",
    "        \n",
    "        # Caracter√≠sticas num√©ricas base\n",
    "        base_numeric_features = [\n",
    "            'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "            'month_sin', 'month_cos', 'spatial_cluster', 'temporal_weight',\n",
    "            'crime_count_7d', 'crime_count_30d', 'crime_severity_score',\n",
    "            'zone_crime_density', 'beat_crime_density', 'location_type_frequency',\n",
    "            'distance_to_center', 'nighttime_activity', 'is_weekend',\n",
    "            'report_delay_hours', 'incident_duration_hours', 'media_attention',\n",
    "            'FireArmInvolved_binary', 'IsBiasMotivationInvolved_binary',\n",
    "            'Vic_Count', 'Zone_int'\n",
    "        ]\n",
    "        \n",
    "        # Agregar coordenadas si est√°n disponibles\n",
    "        if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "            base_numeric_features.extend(['Latitude', 'Longitude'])\n",
    "        \n",
    "        # Caracter√≠sticas categ√≥ricas codificadas\n",
    "        categorical_features = [col for col in df.columns if col.endswith('_encoded')]\n",
    "        \n",
    "        # Combinar todas las caracter√≠sticas\n",
    "        all_features = base_numeric_features + categorical_features\n",
    "        \n",
    "        # Filtrar caracter√≠sticas que realmente existen en el dataset\n",
    "        available_features = [col for col in all_features if col in df.columns]\n",
    "        \n",
    "        # Asegurar que tenemos la variable objetivo\n",
    "        if 'NIBRS_Offense_encoded' not in df.columns:\n",
    "            raise ValueError(\"Variable objetivo 'NIBRS_Offense_encoded' no encontrada\")\n",
    "        \n",
    "        # Preparar datos\n",
    "        feature_cols = available_features\n",
    "\n",
    "        # Evitar que la variable objetivo est√© duplicada en las caracter√≠sticas\n",
    "        if 'NIBRS_Offense_encoded' in feature_cols:\n",
    "            feature_cols.remove('NIBRS_Offense_encoded')\n",
    "        \n",
    "        df_clean = df[feature_cols + ['NIBRS_Offense_encoded']].copy()\n",
    "\n",
    "        \n",
    "        # Rellenar valores faltantes\n",
    "        for col in df_clean.columns:\n",
    "            if col != 'NIBRS_Offense_encoded':\n",
    "                if df_clean[col].dtype in ['float64', 'int64']:\n",
    "                    df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "                else:\n",
    "                    df_clean[col] = df_clean[col].fillna(0)\n",
    "        \n",
    "        # Preparar X e y\n",
    "        X = df_clean[feature_cols]\n",
    "        y = df_clean['NIBRS_Offense_encoded'].astype(int)\n",
    "        \n",
    "        print(f\"Caracter√≠sticas preparadas: {X.shape[1]} variables\")\n",
    "        print(f\"Clases objetivo: {len(np.unique(y))} tipos de crimen diferentes\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_fusion_model(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Entrenar modelo de fusi√≥n optimizado\n",
    "        \"\"\"\n",
    "        print(\"Entrenando modelo de fusi√≥n h√≠brido optimizado...\")\n",
    "        \n",
    "        # Normalizar caracter√≠sticas\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Modelo Random Forest optimizado para clasificaci√≥n multiclase\n",
    "        self.fusion_model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=3,\n",
    "            max_features='sqrt',\n",
    "            bootstrap=True,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.fusion_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Calcular importancia de caracter√≠sticas\n",
    "        feature_importance = self.fusion_model.feature_importances_\n",
    "        self.attention_weights = dict(zip(X_train.columns, feature_importance))\n",
    "        \n",
    "        print(\"Modelo h√≠brido entrenado exitosamente\")\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Realizar predicciones con intervalos de confianza\n",
    "        \"\"\"\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        predictions = self.fusion_model.predict(X_test_scaled)\n",
    "        probabilities = self.fusion_model.predict_proba(X_test_scaled)\n",
    "        \n",
    "        # Calcular confianza de predicci√≥n\n",
    "        confidence_scores = np.max(probabilities, axis=1)\n",
    "        \n",
    "        return predictions, probabilities, confidence_scores\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluaci√≥n completa del modelo\n",
    "        \"\"\"\n",
    "        print(\"Evaluando rendimiento del modelo...\")\n",
    "        \n",
    "        predictions, probabilities, confidence_scores = self.predict(X_test)\n",
    "        \n",
    "        # M√©tricas de clasificaci√≥n\n",
    "        precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "        accuracy = self.fusion_model.score(self.scaler.transform(X_test), y_test)\n",
    "        \n",
    "        self.performance_metrics = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'mean_confidence': np.mean(confidence_scores)\n",
    "        }\n",
    "        \n",
    "        print(f\"Precisi√≥n: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        print(f\"Exactitud: {accuracy:.4f}\")\n",
    "        print(f\"Confianza promedio: {np.mean(confidence_scores):.4f}\")\n",
    "\n",
    "        print(\"\\n=== Classification Report ===\")\n",
    "        print(classification_report(y_test, predictions, zero_division=0))\n",
    "        \n",
    "        # Matriz de confusi√≥n normalizada\n",
    "        cm = confusion_matrix(y_test, predictions, normalize='true')\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, cmap='Blues')\n",
    "        plt.title(\"Matriz de Confusi√≥n Normalizada\")\n",
    "        plt.xlabel(\"Predicho\")\n",
    "        plt.ylabel(\"Real\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self.performance_metrics\n",
    "    \n",
    "    def visualize_results(self, df):\n",
    "        \"\"\"\n",
    "        Visualizaciones comprehensivas de los resultados\n",
    "        \"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(20, 15))\n",
    "            \n",
    "            # Subplot 1: Mapa de calor de cr√≠menes\n",
    "            plt.subplot(3, 4, 1)\n",
    "            if 'Longitude' in df.columns and 'Latitude' in df.columns:\n",
    "                plt.scatter(df['Longitude'], df['Latitude'], \n",
    "                           c=df['spatial_cluster'], cmap='viridis', alpha=0.6, s=0.5)\n",
    "                plt.colorbar()\n",
    "                plt.title('Clusters Espaciales de Cr√≠menes')\n",
    "                plt.xlabel('Longitud')\n",
    "                plt.ylabel('Latitud')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Coordenadas no disponibles', ha='center', va='center')\n",
    "                plt.title('Clusters Espaciales')\n",
    "            \n",
    "            # Subplot 2: Distribuci√≥n temporal por hora\n",
    "            plt.subplot(3, 4, 2)\n",
    "            hourly_crimes = df.groupby('hour').size()\n",
    "            plt.bar(hourly_crimes.index, hourly_crimes.values, alpha=0.7)\n",
    "            plt.title('Distribuci√≥n de Cr√≠menes por Hora')\n",
    "            plt.xlabel('Hora del D√≠a')\n",
    "            plt.ylabel('N√∫mero de Cr√≠menes')\n",
    "            \n",
    "            # Subplot 3: Cr√≠menes por d√≠a de la semana\n",
    "            plt.subplot(3, 4, 3)\n",
    "            daily_crimes = df.groupby('day_of_week').size()\n",
    "            days = ['Lun', 'Mar', 'Mi√©', 'Jue', 'Vie', 'S√°b', 'Dom']\n",
    "            plt.bar(range(len(daily_crimes)), daily_crimes.values, alpha=0.7)\n",
    "            plt.xticks(range(len(daily_crimes)), [days[i] for i in daily_crimes.index])\n",
    "            plt.title('Cr√≠menes por D√≠a de la Semana')\n",
    "            plt.xlabel('D√≠a de la Semana')\n",
    "            plt.ylabel('N√∫mero de Cr√≠menes')\n",
    "            \n",
    "            # Subplot 4: Top tipos de crimen\n",
    "            plt.subplot(3, 4, 4)\n",
    "            if 'NIBRS_Offense' in df.columns:\n",
    "                top_crimes = df['NIBRS_Offense'].value_counts().head(8)\n",
    "                plt.barh(range(len(top_crimes)), top_crimes.values)\n",
    "                plt.yticks(range(len(top_crimes)), [str(x)[:20] for x in top_crimes.index], fontsize=8)\n",
    "                plt.title('Top 8 Tipos de Crimen')\n",
    "                plt.xlabel('Frecuencia')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Tipos de crimen no disponibles', ha='center', va='center')\n",
    "                plt.title('Tipos de Crimen')\n",
    "            \n",
    "            # Subplot 5: Distribuci√≥n de severidad\n",
    "            plt.subplot(3, 4, 5)\n",
    "            plt.hist(df['crime_severity_score'], bins=30, alpha=0.7)\n",
    "            plt.title('Distribuci√≥n de Severidad del Crimen')\n",
    "            plt.xlabel('Score de Severidad')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            \n",
    "            # Subplot 6: Pesos de atenci√≥n temporal\n",
    "            plt.subplot(3, 4, 6)\n",
    "            plt.hist(df['temporal_weight'], bins=50, alpha=0.7)\n",
    "            plt.title('Pesos de Atenci√≥n Temporal')\n",
    "            plt.xlabel('Peso de Atenci√≥n')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            \n",
    "            # Subplot 7: Cr√≠menes por zona\n",
    "            plt.subplot(3, 4, 7)\n",
    "            if 'Zone' in df.columns:\n",
    "                zone_crimes = df.groupby('Zone').size().sort_values(ascending=False).head(10)\n",
    "                plt.bar(range(len(zone_crimes)), zone_crimes.values, alpha=0.7)\n",
    "                plt.xticks(range(len(zone_crimes)), zone_crimes.index, rotation=45, fontsize=8)\n",
    "                plt.title('Cr√≠menes por Zona (Top 10)')\n",
    "                plt.ylabel('N√∫mero de Cr√≠menes')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Zonas no disponibles', ha='center', va='center')\n",
    "                plt.title('Cr√≠menes por Zona')\n",
    "            \n",
    "            # Subplot 8: Armas de fuego involucradas\n",
    "            plt.subplot(3, 4, 8)\n",
    "            firearm_dist = df['FireArmInvolved_binary'].value_counts()\n",
    "            labels = ['No', 'S√≠']\n",
    "            plt.pie(firearm_dist.values, labels=labels, autopct='%1.1f%%')\n",
    "            plt.title('Armas de Fuego Involucradas')\n",
    "            \n",
    "            # Subplot 9: Tendencia temporal mensual\n",
    "            plt.subplot(3, 4, 9)\n",
    "            monthly_trend = df.groupby(df['datetime'].dt.to_period('M')).size()\n",
    "            if len(monthly_trend) > 1:\n",
    "                plt.plot(range(len(monthly_trend)), monthly_trend.values)\n",
    "                plt.title('Tendencia Mensual de Cr√≠menes')\n",
    "                plt.xlabel('Per√≠odo')\n",
    "                plt.ylabel('N√∫mero de Cr√≠menes')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Datos insuficientes para tendencia', ha='center', va='center')\n",
    "                plt.title('Tendencia Mensual')\n",
    "            \n",
    "            # Subplot 10: Actividad por clusters\n",
    "            plt.subplot(3, 4, 10)\n",
    "            cluster_activity = df.groupby('spatial_cluster').size()\n",
    "            plt.bar(cluster_activity.index, cluster_activity.values, alpha=0.7)\n",
    "            plt.title('Actividad por Cluster Espacial')\n",
    "            plt.xlabel('Cluster')\n",
    "            plt.ylabel('N√∫mero de Cr√≠menes')\n",
    "            \n",
    "            # Subplot 11: Actividad nocturna vs diurna\n",
    "            plt.subplot(3, 4, 11)\n",
    "            night_activity = df['nighttime_activity'].value_counts()\n",
    "            labels = ['Diurno', 'Nocturno']\n",
    "            plt.pie(night_activity.values, labels=labels, autopct='%1.1f%%')\n",
    "            plt.title('Actividad Nocturna vs Diurna')\n",
    "            \n",
    "            # Subplot 12: Importancia de caracter√≠sticas\n",
    "            plt.subplot(3, 4, 12)\n",
    "            if self.attention_weights:\n",
    "                importance_df = pd.DataFrame(\n",
    "                    list(self.attention_weights.items()),\n",
    "                    columns=['Feature', 'Importance']\n",
    "                ).sort_values('Importance', ascending=False).head(10)\n",
    "                \n",
    "                plt.barh(range(len(importance_df)), importance_df['Importance'].values)\n",
    "                plt.yticks(range(len(importance_df)), \n",
    "                          [str(x)[:15] for x in importance_df['Feature'].values], fontsize=8)\n",
    "                plt.title('Top 10 Caracter√≠sticas Importantes')\n",
    "                plt.xlabel('Importancia')\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'Modelo no entrenado', ha='center', va='center')\n",
    "                plt.title('Importancia de Caracter√≠sticas')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en visualizaci√≥n: {e}\")\n",
    "            print(\"Continuando sin visualizaciones...\")\n",
    "    def prepare_hybrid_training_data(self, df):\n",
    "        \"\"\"\n",
    "        Prepara los datos para el entrenamiento h√≠brido:\n",
    "        - X_context: caracter√≠sticas contextuales (entrenadas con RandomForest)\n",
    "        - embeddings: representaciones embebidas de series temporales (para RNN)\n",
    "        - y_context: etiquetas a predecir\n",
    "    \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame procesado con caracter√≠sticas contextuales y temporales\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, pd.Series]: X_context, embeddings, y_context\n",
    "        \"\"\"\n",
    "        # Verificar que temporal_series exista\n",
    "        if 'temporal_series' not in df.columns:\n",
    "            raise ValueError(\"Falta la columna 'temporal_series'. Aseg√∫rate de que 'advanced_temporal_analysis' la haya generado.\")\n",
    "    \n",
    "        # === Construir embeddings desde temporal_series ===\n",
    "        embeddings = df['temporal_series'].apply(lambda ts: np.array(ts)).tolist()\n",
    "        embeddings = pd.DataFrame(embeddings)\n",
    "    \n",
    "        # === Etiqueta objetivo ===\n",
    "        y_context = df['NIBRS_Offense_encoded']\n",
    "    \n",
    "        # === Seleccionar y limpiar caracter√≠sticas contextuales ===\n",
    "        exclude_cols = ['NIBRS_Offense_encoded', 'temporal_series', 'ReportDate', 'OccurredFromDate', 'OccurredToDate', 'datetime',\n",
    "                        'IncidentNumber', 'ReportNumber', 'ChargeId']\n",
    "        \n",
    "        X_context = df.drop(columns=[col for col in exclude_cols if col in df.columns], errors='ignore')\n",
    "    \n",
    "        # Codificar columnas categ√≥ricas no num√©ricas (por ejemplo, 'Day_of_the_week' si a√∫n est√° como string)\n",
    "        for col in X_context.select_dtypes(include=['object', 'string']).columns:\n",
    "            if X_context[col].nunique() < 100:\n",
    "                X_context[col] = LabelEncoder().fit_transform(X_context[col].astype(str))\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Columna '{col}' tiene demasiados valores √∫nicos. Considera eliminarla.\")\n",
    "                X_context = X_context.drop(columns=[col])  # Se descarta por seguridad\n",
    "    \n",
    "        return X_context, embeddings, y_context\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Obtener ranking de importancia de caracter√≠sticas\n",
    "        \"\"\"\n",
    "        if self.attention_weights:\n",
    "            importance_df = pd.DataFrame(\n",
    "                list(self.attention_weights.items()),\n",
    "                columns=['Feature', 'Importance']\n",
    "            ).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            return importance_df\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac15f4df-285e-4f2c-a72f-f9a103258604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_with_real_data(csv_file_path):\n",
    "    print(\"=== Sistema de Predicci√≥n Criminal con Datos Reales ===\\n\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Cargando datos desde: {csv_file_path}\")\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        print(f\"Dataset cargado: {df.shape[0]} registros, {df.shape[1]} campos\\n\")\n",
    "        df['OccurredFromDate'] = pd.to_datetime(df['OccurredFromDate'], errors='coerce')\n",
    "        df['OccurredToDate'] = pd.to_datetime(df['OccurredToDate'], errors='coerce')\n",
    "\n",
    "        print(\"Primeras columnas del dataset:\")\n",
    "        print(df.columns.tolist()[:10])\n",
    "        print(f\"\\nTama√±o del dataset: {df.shape}\")\n",
    "        print(f\"Rango de fechas: {df['OccurredFromDate'].min()} a {df['OccurredFromDate'].max()}\")\n",
    "\n",
    "        model = AdaptedCrimePredictionModel(\n",
    "            spatial_clusters=20,\n",
    "            temporal_window=30,\n",
    "            attention_features=15\n",
    "        )\n",
    "\n",
    "        print(\"\\nIniciando pipeline de procesamiento...\\n\")\n",
    "\n",
    "        df_processed = model.preprocess_real_data(df)\n",
    "        df_severity = model.create_crime_severity_score(df_processed)\n",
    "        df_clustered = model.enhanced_spatial_clustering(df_severity)\n",
    "        df_temporal = model.advanced_temporal_analysis(df_clustered)\n",
    "        df_contextual = model.create_contextual_features(df_temporal)\n",
    "\n",
    "        # === Crear secuencias para RNN ===\n",
    "        X_seq, y_seq = model.create_temporal_sequences(df_contextual, model.temporal_window)\n",
    "        if len(X_seq) == 0:\n",
    "            raise ValueError(\"No se pudieron crear secuencias temporales. Verifica tus datos.\")\n",
    "        \n",
    "        # === Entrenar RNN y extraer embeddings ===\n",
    "        rnn_embeddings = model.train_rnn_model(X_seq, y_seq)\n",
    "\n",
    "        # === Preparar caracter√≠sticas contextuales ===\n",
    "        X_context, _, y_context = model.prepare_hybrid_training_data(df_contextual)\n",
    "        valid_indices = model._get_valid_sequence_indices(df_contextual, model.temporal_window)\n",
    "        X_context = X_context.iloc[valid_indices].reset_index(drop=True)\n",
    "        y_context = pd.Series(y_context).iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "        # Alinear embeddings\n",
    "        embeddings = pd.DataFrame(rnn_embeddings)\n",
    "\n",
    "        # Diagn√≥stico de clases\n",
    "        class_counts = y_context.value_counts().sort_index()\n",
    "        print(\"Distribuci√≥n de clases:\")\n",
    "        print(class_counts)\n",
    "\n",
    "        imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "        print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\n",
    "        plt.title(\"Distribuci√≥n de clases en 'NIBRS_Offense_encoded'\")\n",
    "        plt.xlabel(\"Clase (tipo de crimen codificado)\")\n",
    "        plt.ylabel(\"N√∫mero de registros\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Filtrar clases con al menos 2 muestras\n",
    "        valid_classes = class_counts[class_counts >= 2].index\n",
    "        mask = y_context.isin(valid_classes)\n",
    "        X_context = X_context[mask].reset_index(drop=True)\n",
    "        embeddings = embeddings[mask].reset_index(drop=True)\n",
    "        y_context = y_context[mask].reset_index(drop=True)\n",
    "\n",
    "        # Separar en conjunto de entrenamiento y prueba\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        X_train, X_test, emb_train, emb_test, y_train, y_test = train_test_split(\n",
    "            X_context, embeddings, y_context,\n",
    "            test_size=0.2, stratify=y_context, random_state=42\n",
    "        )\n",
    "\n",
    "        print(\"\\nEntrenando modelo combinado...\")\n",
    "        model.train_combined_model(X_train, emb_train, y_train)\n",
    "\n",
    "        print(\"\\nEvaluando modelo combinado...\")\n",
    "        metrics = model.evaluate_combined_model(X_test, emb_test, y_test)\n",
    "\n",
    "        print(\"\\nTop 15 Caracter√≠sticas m√°s Importantes:\")\n",
    "        feature_importance = model.get_feature_importance() if hasattr(model, 'get_feature_importance') else None\n",
    "        if feature_importance is not None:\n",
    "            print(feature_importance.head(15))\n",
    "\n",
    "        print(\"\\n=== Resumen del Modelo ===\")\n",
    "        print(f\"Registros procesados: {len(df_contextual):,}\")\n",
    "        print(f\"Clusters espaciales: {len(np.unique(df_contextual['spatial_cluster']))}\")\n",
    "        print(f\"Precisi√≥n del modelo: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "        print(\"\\n=== Entrenamiento Completado Exitosamente ===\")\n",
    "\n",
    "        # Exportar modelos\n",
    "        export_dir = \"exported_models\"\n",
    "        os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "        joblib.dump(model.fusion_model, os.path.join(export_dir, \"fusion_model.pkl\"))\n",
    "        print(\"‚úÖ Modelo fusion ok.\")\n",
    "        joblib.dump((scaler, X_train.columns.tolist()), \"exported_models/scaler.pkl\")\n",
    "        print(\"‚úÖ Escaler ok.\")\n",
    "        model.rnn_model.save(os.path.join(export_dir, \"rnn_model.h5\"))\n",
    "        model.rnn_model.save(os.path.join(export_dir, \"rnn_model.keras\"))\n",
    "        print(\"‚úÖ rnn ok.\")\n",
    "\n",
    "        print(\"‚úÖ Modelos exportados correctamente.\")\n",
    "\n",
    "        return model, df_contextual, metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el procesamiento: {str(e)}\")\n",
    "        print(\"Verificar que el archivo CSV existe y tiene el formato correcto.\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4098deb-922d-4feb-8993-b0ab48f6e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sistema de Predicci√≥n Criminal con Datos Reales ===\n",
      "\n",
      "Cargando datos desde: AxonCrimeData_Export_WA_1686331975127960619 (1).csv\n",
      "Dataset cargado: 261177 registros, 32 campos\n",
      "\n",
      "Primeras columnas del dataset:\n",
      "['OBJECTID', 'ChargeId', 'IncidentNumber', 'ReportNumber', 'FireArmInvolved', 'ReportDate', 'OccurredFromDate', 'OccurredToDate', 'Day_of_the_week', 'Day_Number']\n",
      "\n",
      "Tama√±o del dataset: (261177, 32)\n",
      "Rango de fechas: 1924-08-21 20:39:00 a 2025-06-18 12:01:00\n",
      "\n",
      "Iniciando pipeline de procesamiento...\n",
      "\n",
      "Iniciando preprocesamiento de datos reales...\n",
      "Datos preprocesados: 261175 registros, 56 caracter√≠sticas\n",
      "Creando score de severidad del crimen...\n",
      "Ejecutando clustering espacial mejorado...\n",
      "Identificados 21 clusters espaciales\n",
      "Ejecutando an√°lisis temporal avanzado...\n",
      "Creando caracter√≠sticas contextuales avanzadas...\n",
      "Creando secuencias temporales para la RNN...\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Reemplazar con la ruta real de tu archivo CSV\n",
    "    csv_path = \"AxonCrimeData_Export_WA_1686331975127960619 (1).csv\"\n",
    "    \n",
    "    # Para usar el c√≥digo, descomenta la siguiente l√≠nea y ajusta la ruta:\n",
    "    trained_model, processed_data, performance_metrics = main_with_real_data(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e9780-1d7c-43cb-8ef8-dd0e53dd9fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd92759-bcd4-46fa-99d5-f4cc53f7b37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499bb88f-f92f-45eb-89da-da2e57c2ba2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7006f-2d37-4739-befd-255b8af18204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
